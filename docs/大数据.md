## 1. 虚拟机环境配置

首先，对于大数据集群提前进行主机名、IP地址、主次服务的规划: 

| 虚拟机名称 | 主机地址       |       |
| ---------- | -------------- | ----- |
| Hadoop102  | 192.168.10.102 | NN DN |
| Hadoop103  | 192.168.10.103 | DN    |
| Hadoop104  | 192.168.10.104 | DN SN |

查看 Hadoop 目录结构

```bash
cd /opt/module/hadoop-3.1.3/
ll
```

### 1.1 本地运行模式（官方 WordCount）

创建在 hadoop-3.1.3 文件下面创建一个 wcinput 文件夹

```bash
mkdir wcinput
```

在 wcinput 文件下创建一个 word.txt文件

```bash
cd wcinput
vim word.txt
```

编辑word.txt文件

```
hadoop yarn
hadoop mapreduce
itwork
itwork
```

回到 Hadoop 目录/opt/module/hadoop-3.1.3，执行程序

```bash
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput
```

查看结果

```bash
cat wcoutput/part-r-00000
```

看到如下结果：

```
hadoop  2
itwork  2
mapreduce       1
yarn    1
```

### 1.2 完全分布式运行模式（重点）

编写集群分发脚本 xsync

scp命令语法：

| 命令 | 递归 | 要拷贝的文件路径/名称 | 目的地用户@主机:目的地路径/名称 |
| ---- | ---- | --------------------- | ------------------------------- |
| scp  | -r   | $pdir/$fname          | $user@$host:$pdir/$fname        |

1. 在 hadoop102、hadoop103、hadoop104 都已经创建好的/opt/module、  /opt/software 两个目录，并且已经把这两个目录修改为`用户名:用户名`

![](https://i.wpic.cc/g/2023/03/31/6426377adf999.png)

![](https://i.wpic.cc/g/2023/03/31/642637096bc3d.png)

2. 在 hadoop102 上，将 hadoop102 中/opt/module/jdk1.8.0_212 目录拷贝到 hadoop103 上。

   ```bash
   [lucp@Hadoop102 ~]$ scp -r /opt/module/jdk1.8.0_212 lucp@hadoop103:/opt/module
   ```

![](https://i.wpic.cc/g/2023/03/31/642639e0e5a0e.png)

​	![](https://i.wpic.cc/g/2023/03/31/64263a396ac23.png)

3. 在 hadoop103 上，将 hadoop102 中/opt/module/hadoop-3.1.3 目录拷贝到 hadoop103 上。

   ```bash
   [lucp@Hadoop103 ~]$ scp -r lucp@hadoop102:/opt/module/hadoop-3.1.3 /opt/module/
   ```

   ![](https://i.wpic.cc/g/2023/03/31/6426473abc7a6.png)

4. 在 hadoop103 上操作，将 hadoop102 中/opt/module 目录下所有目录拷贝到 hadoop104 上。

   ```bash
   [lucp@Hadoop103 ~]$ scp -r lucp@hadoop102:/opt/module/* lucp@hadoop104:/opt/module
   ```

   ![](https://i.wpic.cc/g/2023/03/31/642646d876166.png)

rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 

rsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更 新。scp 是把所有文件都复制过去。

| 命令  | 选项参数 | 要拷贝的文件路径/名称 | 目的地用户@主机:目的地路径/名称 |
| ----- | -------- | --------------------- | ------------------------------- |
| rsync | -av      | $pdir/$fname          | $user@$host:$pdir/$fname        |

删除 hadoop103 中`/opt/module/hadoop-3.1.3/wcinput`

```bash
[lucp@Hadoop103 hadoop-3.1.3]$ rm -rf wcinput/
```

同步 hadoop102 中的/opt/module/hadoop-3.1.3 到 hadoop103

```BASH
[lucp@Hadoop102 module]$ rsync -av hadoop-3.1.3/ lucp@hadoop103:/opt/module/hadoop-3.1.3/
```

![](https://i.wpic.cc/g/2023/03/31/64264ce86174e.png)

![](https://i.wpic.cc/g/2023/03/31/64264c6eb4a5f.png)

❤xsync 集群分发脚本

```bash
[lucp@Hadoop102 ~]$ pwd
/home/lucp
[lucp@Hadoop102 ~]$ mkdir bin
[lucp@Hadoop102 ~]$ cd bin
[lucp@Hadoop102 bin]$ vim xsync
```

在该文件中编写如下代码 

```
#!/bin/bash

#1. 判断参数个数
if [ $# -lt 1 ]
then
	echo Not Enough Arguement!
	exit;
fi

#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
 echo ==================== $host ====================
 
 #3. 遍历所有目录，挨个发送
 for file in $@
	do
 
 #4. 判断文件是否存在
 if [ -e $file ]
	then
 
 #5. 获取父目录
 pdir=$(cd -P $(dirname $file); pwd)
 
 #6. 获取当前文件的名称
 fname=$(basename $file)
 ssh $host "mkdir -p $pdir"
 rsync -av $pdir/$fname $host:$pdir
 else
 echo $file does not exists!
 fi
 done
done
```

修改脚本 xsync 具有执行权限

```bash
[lucp@Hadoop102 bin]$ chmod 777 xsync
# -rwxrwxrwx. 1 lucp lucp 567 3月  31 10:48 xsync
[lucp@Hadoop102 ~]$ xsync /home/lucp(用户名)/bin
```

![](https://i.wpic.cc/g/2023/03/31/6426501e59c3a.png)



```bash
[lucp@Hadoop102 /]$ cd ~/bin
[lucp@Hadoop102 bin]$ pwd
# /home/lucp/bin
[lucp@Hadoop102 bin]$ sudo cp xsync /bin/ 
[lucp@Hadoop102 ~]$ sudo ./bin/xsync /etc/profile.d/my_env.sh
```

```bash
[lucp@hadoop102 ~]$ cd ~/.ssh
[lucp@hadoop102 .ssh]$ pwd
# /home/lucp/.ssh
[lucp@hadoop102 .ssh]$ ssh-keygen -t rsa
# 三下回车
```

![](https://i.wpic.cc/g/2023/03/31/642674143f64b.png)

```BASH
[lucp@hadoop102 .ssh]$ ssh-copy-id hadoop102
[lucp@hadoop102 .ssh]$ ssh-copy-id hadoop103
[lucp@hadoop102 .ssh]$ ssh-copy-id hadoop104
```

还需要在 hadoop103 上采用用户账号配置一下无密登录到 hadoop102、hadoop103、 hadoop104 服务器上。 还需要在 hadoop104 上采用用户账号配置一下无密登录到 hadoop102、hadoop103、 hadoop104 服务器上。

![](https://i.wpic.cc/g/2023/03/31/6426754046f86.png)

```bash
[lucp@hadoop102 .ssh]$ ssh hadoop103
Last login: Sun Apr  2 21:25:41 2023 from 192.168.10.1
[lucp@hadoop103 ~]$ 登出
Connection to hadoop103 closed.
[lucp@hadoop102 .ssh]$ ssh hadoop104
Last login: Sun Apr  2 21:31:45 2023 from 192.168.10.1
[lucp@hadoop104 ~]$ 登出
Connection to hadoop104 closed.
[lucp@hadoop102 .ssh]$ ssh hadoop102
Last login: Sun Apr  2 21:31:41 2023 from 192.168.10.1
[lucp@hadoop102 ~]$ 登出
Connection to hadoop102 closed.
```

```bash
# 在集群上分发配置好的 Hadoop 配置文件
[lucp@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop
```

![image-20230331145727377](C:\Users\Lucp\AppData\Roaming\Typora\typora-user-images\image-20230331145727377.png)

```bash
# 去 103 和 104 上查看文件分发情况
[lucp@hadoop103 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml 
[lucp@Hadoop104 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml
# 配置 workers
[lucp@hadoop102 hadoop]$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers
# 同步所有节点配置文件
[lucp@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/
# 启动 HDFS
[lucp@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh
```



### 1.3 集群配置

1. 配置 core-site.xml `etc/hadoop/core-site.xml:`

   ```bash
   [lucp@hadoop102 hadoop]$ cd $HADOOP_HOME/etc/hadoop
   [lucp@hadoop102 hadoop]$ vim core-site.xml
   ```

   ```xml
   <configuration>
    <!-- 指定 NameNode 的地址 -->
    <property>
    <name>fs.defaultFS</name>
    <value>hdfs://hadoop102:8020</value>
    </property>
    <!-- 指定 hadoop 数据的存储目录 -->
    <property>
    <name>hadoop.tmp.dir</name>
    <value>/opt/module/hadoop-3.1.3/data</value>
    </property>
    <!-- 配置 HDFS 网页登录使用的静态用户 -->
    <property>
    <name>hadoop.http.staticuser.user</name>
    <value>lucp（用户名）</value>
    </property>
   </configuration>
   ```

2. 配置 hdfs-site.xml `etc/hadoop/hdfs-site.xml:`

   ```bash
   [lucp@hadoop102 hadoop]$ vim hdfs-site.xml
   ```

   ```xml
   <configuration>
   <!-- nn web 端访问地址-->
   <property>
    <name>dfs.namenode.http-address</name>
    <value>hadoop102:9870</value>
    </property>
   <!-- 2nn web 端访问地址-->
    <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>hadoop104:9868</value>
    </property>
   </configuration>
   ```

3. 配置 mapred-site.xml ``etc/hadoop/mapred-site.xml`:`

   ```bash
   [lucp@hadoop102 hadoop]$ vim mapred-site.xml
   ```

   ```xml
   <configuration>
   <!-- 指定 MapReduce 程序运行在 Yarn 上 -->
   	<property>
   		<name>mapreduce.framework.name</name>
    		<value>yarn</value>
   	</property>
   </configuration>
   ```

4. 配置 yarn-site.xml ``etc/hadoop/yarn-site.xml`:`

   ```bash
   [lucp@hadoop102 hadoop]$ vim yarn-site.xml
   ```

   ```xml
   <property>
   	<name>yarn.nodemanager.aux-services</name>
   	<value>mapreduce_shuffle</value>
   </property>
   <property>
   	<name>yarn.resourcemanager.hostname</name>
   	<value>hadoop103</value>
   </property>
   ```

5. 配置 workers

   ```bash
   [lucp@hadoop102 hadoop]$ vim workers
   [lucp@hadoop102 hadoop]$ sudo vim /etc/hosts
   ```

   ```apl
   192.168.10.100 hadoop100
   192.168.10.101 hadoop101
   192.168.10.102 hadoop102
   192.168.10.103 hadoop103
   192.168.10.104 hadoop104
   ```

   > Vim中dd删除

6. 克隆其他集群信息
       关闭机器，准备克隆
       克隆后，修改Hadoop103、Hadoop103的IP和主机名

   ```bash
   [lucp@hadoop103 ~]$ sudo vim /etc/sysconfig/network-scripts/ifcfg-ens33
   [lucp@hadoop103 ~]$ sudo vim /etc/hostname
   [lucp@hadoop104 ~]$ sudo vim /etc/sysconfig/network-scripts/ifcfg-ens33
   [lucp@hadoop104 ~]$ sudo vim /etc/hostname
   ```

   ```
   IPADDR=192.168.10.103
   hadoop103
   IPADDR=192.168.10.104
   hadoop104
   ```

7. 配置集群的ssh免密

   ```bash
   [lucp@hadoop102 ~]$ cd ~/.ssh
   [lucp@hadoop102 .ssh]$ pwd
   # /home/lucp/.ssh
   [lucp@hadoop102 .ssh]$ ssh-keygen -t rsa
   # 三下回车
   
   [lucp@hadoop102 .ssh]$ ssh-copy-id hadoop102
   [lucp@hadoop102 .ssh]$ ssh-copy-id hadoop103
   [lucp@hadoop102 .ssh]$ ssh-copy-id hadoop104
   # 在其他两台虚拟机上执行相同操作
   ```

   ![](https://i.wpic.cc/g/2023/03/31/6426754046f86.png)

   ```bash
   [lucp@hadoop102 .ssh]$ ssh hadoop103
   Last login: Sun Apr  2 21:25:41 2023 from 192.168.10.1
   [lucp@hadoop103 ~]$ 登出
   Connection to hadoop103 closed.
   [lucp@hadoop102 .ssh]$ ssh hadoop104
   Last login: Sun Apr  2 21:31:45 2023 from 192.168.10.1
   [lucp@hadoop104 ~]$ 登出
   Connection to hadoop104 closed.
   [lucp@hadoop102 .ssh]$ ssh hadoop102
   Last login: Sun Apr  2 21:31:41 2023 from 192.168.10.1
   [lucp@hadoop102 ~]$ 登出
   Connection to hadoop102 closed.
   ```

8. 启动集群

   ```bash
   [lucp@hadoop102 ~]$ hdfs namenode -format
   [lucp@hadoop102 ~]$ start-dfs.sh
   ```

   ```bash
   Starting namenodes on [hadoop102]
   hadoop102: namenode is running as process 12265.  Stop it first.
   Starting datanodes
   hadoop103: datanode is running as process 10523.  Stop it first.
   hadoop104: datanode is running as process 10467.  Stop it first.
   Starting secondary namenodes [hadoop104]
   hadoop104: secondarynamenode is running as process 10672.  Stop it first.
   ```

   > [无法获取"/opt/moudle/hadoop-3.1.3/logs/hadoop-xxx-nodemanager-hadoop103.out.2" 的文件状态(stat): 没有那个文件或目录](https://www.cnblogs.com/zhy-god/p/14986534.html)解决办法

   

   ```bash
   [lucp@hadoop102 ~]$ jps
   12452 DataNode
   12265 NameNode
   13723 Jps
   ```

   ```bash
   [lucp@hadoop103 .ssh]$ jps
   10523 DataNode
   10829 Jps
   ```

   ```bash
   [lucp@hadoop104 .ssh]$ jps
   10672 SecondaryNameNode
   10467 DataNode
   10947 Jps
   ```

   >  jps是jdk提供的一个查看当前java进程的小工具

   Web 端查看 HDFS 的 NameNode：浏览器中输入：http://hadoop102:9870

   ![image-20230402221905812](C:\Users\Lucp\AppData\Roaming\Typora\typora-user-images\image-20230402221905812.png)

   ```bash
   [lucp@hadoop103 .ssh]$ start-yarn.sh
   ```

   Web 端查看 YARN 的 ResourceManager：浏览器中输入：http://hadoop103:8088

### 1.4 集群基本测试

1. 上传文件到集群

   ```bash
   [lucp@hadoop102 ~]$ hadoop fs -mkdir /wcinput
   [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -put wcinput/word.txt /wcinput
   [lucp@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /wcinput /wcoutput
   ```

   ![](https://i.postimg.cc/jqbjg1hN/1680501097764.png?dl=1)

### 1.5 配置历史服务器

1. 配置 mapred-site.xml

   ```bash
   [lucp@hadoop102 hadoop]$ cd $HADOOP_HOME/etc/hadoop
   [lucp@hadoop102 hadoop]$ vim mapred-site.xml
   ```

   向内增加下面

   ```xml
   <!-- 历史服务器端地址 -->
   <property>
   	<name>mapreduce.jobhistory.address</name>
   	<value>hadoop102:10020</value>
   </property>
   <!-- 历史服务器 web 端地址 -->
   <property>
   	<name>mapreduce.jobhistory.webapp.address</name>
   	<value>hadoop102:19888</value>
   </property>
   ```

2. 分发配置

   ```bash
   [lucp@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/mapred-site.xml
   ```

   ```bash
   ==================== hadoop102 ====================
   sending incremental file list
   
   sent 66 bytes  received 12 bytes  156.00 bytes/sec
   total size is 1,169  speedup is 14.99
   ==================== hadoop103 ====================
   sending incremental file list
   mapred-site.xml
   
   sent 586 bytes  received 47 bytes  1,266.00 bytes/sec
   total size is 1,169  speedup is 1.85
   ==================== Hadoop104 ====================
   sending incremental file list
   mapred-site.xml
   
   sent 586 bytes  received 47 bytes  1,266.00 bytes/sec
   total size is 1,169  speedup is 1.85
   ```

3. 在 hadoop102 启动历史服务器

   ```bash
   [lucp@hadoop102 hadoop]$ mapred --daemon start historyserver
   [lucp@hadoop102 hadoop]$ jps
   ```

   ```bash
   13191 Jps
   10588 NodeManager
   10142 NameNode
   13167 JobHistoryServer
   10271 DataNode
   ```

4. 查看 JobHistory：[JobHistory](http://hadoop102:19888/jobhistory)

### 1.6配置日志的聚集

1. 配置 mapred-site.xml

   ```bash
   [lucp@hadoop102 hadoop]$ vim yarn-site.xml
   ```

   

   ```xml
   <!-- 开启日志聚集功能 -->
   <property>
   	<name>yarn.log-aggregation-enable</name>
   	<value>true</value>
   </property>
   <!-- 设置日志聚集服务器地址 -->
   <property> 
   	<name>yarn.log.server.url</name> 
   	<value>http://hadoop102:19888/jobhistory/logs</value>
   </property>
   <!-- 设置日志保留时间为 7 天 -->
   <property>
   	<name>yarn.log-aggregation.retain-seconds</name>
   	<value>604800</value>
   </property>
   ```

2. 分发配置

   ```bash
   [lucp@hadoop102 hadoop]$ xsync yarn-site.xml
   ==================== hadoop102 ====================
   sending incremental file list
   
   sent 63 bytes  received 12 bytes  150.00 bytes/sec
   total size is 1,565  speedup is 20.87
   ==================== hadoop103 ====================
   sending incremental file list
   yarn-site.xml
   
   sent 1,675 bytes  received 47 bytes  3,444.00 bytes/sec
   total size is 1,565  speedup is 0.91
   ==================== Hadoop104 ====================
   sending incremental file list
   yarn-site.xml
   
   sent 1,675 bytes  received 47 bytes  3,444.00 bytes/sec
   total size is 1,565  speedup is 0.91
   ```

3. 关闭 NodeManager 、ResourceManager 和 HistoryServer

   ```bash
   [lucp@hadoop102 hadoop]$ mapred --daemon stop historyserver
   [lucp@hadoop102 hadoop]$ jps
   13588 Jps
   10588 NodeManager
   10142 NameNode
   10271 DataNode
   ```

   ```bash
   [lucp@hadoop103 ~]$ stop-yarn.sh
   ```

4. 启动 NodeManager 、ResourceManage 和 HistoryServer

   ```bash
   [lucp@hadoop102 hadoop]$ mapred --daemon start historyserver
   [lucp@hadoop102 hadoop]$ jps
   13984 NodeManager
   14295 Jps
   10142 NameNode
   14143 JobHistoryServer
   10271 DataNode
   
   [lucp@hadoop103 ~]$ start-yarn.sh
   ```

5. 删除 HDFS 上已经存在的输出文件

   ```bash
   [lucp@hadoop102 ~]$ hadoop fs -rm -r /wcoutput
   Deleted /wcoutput
   ```

6. 执行 WordCount 程序

   ```bash
   [lucp@hadoop102 ~]$ cd /opt/module/hadoop-3.1.3/
   [lucp@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /wcinput /wcoutput
   ```

   ![image-20230403141725823](https://i.postimg.cc/DFpwJFmW/1680502652048.png?dl=1)

   ![image-20230403142930183](https://i.postimg.cc/NYfB80ns/1680503372253.png?dl=1)

7. 查看任务运行日志

   ![image-20230403143354634](https://i.postimg.cc/51K5bnQw/1680503658752.png?dl=1)

   ![image-20230403143547963](https://i.postimg.cc/w9rTBRc4/1680503771241.png?dl=1)



### 1.7 集群启动/停止方式总结

1. 各个模块分开启动/停止（配置 ssh 是前提）**常用**

   1. 整体启动/停止 HDFS

      ```
      start-dfs.sh/stop-dfs.sh
      ```

   2. 整体启动/停止 YARN

      ```
      start-yarn.sh/stop-yarn.sh
      ```

2. 各个服务组件逐一启动/停止

   1. 分别启动/停止 HDFS 组件

      ```
      hdfs --daemon start/stop namenode/datanode/secondarynamenode
      ```

   2. 启动/停止 YARN

      ```
      yarn --daemon start/stop resourcemanager/nodemanager
      ```

### 1.8 编写 Hadoop 集群常用脚本

1. Hadoop 集群启停脚本（包含 HDFS，Yarn，Historyserver）：myhadoop.sh

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$ cd ~/bin
   [lucp@hadoop102 bin]$ vim myhadoop.sh
   ```

   ```sh
   #!/bin/bash
   if [ $# -lt 1 ]
   then
   	echo "No Args Input..."
   	exit ;
   fi
   case $1 in
   
   "start")
    echo " =================== 启动 hadoop 集群 ==================="
    echo " --------------- 启动 hdfs ---------------"
    ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
    echo " --------------- 启动 yarn ---------------"
    ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
   echo " --------------- 启动 historyserver ---------------"
   ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
   ;;
   
   "stop")
    echo " =================== 关闭 hadoop 集群 ==================="
   echo " --------------- 关闭 historyserver ---------------"
   ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
    echo " --------------- 关闭 yarn ---------------"
    ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
    echo " --------------- 关闭 hdfs ---------------"
    ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
   ;;
   
   *)
    echo "Input Args Error..."
   ;;
   esac
   ```

   ```bash
   [lucp@hadoop102 bin]$ chmod 777 myhadoop.sh
   ```

2. 查看三台服务器 Java 进程脚本：jpsall

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$ cd ~/bin
   [lucp@hadoop102 bin]$ vim jpsall
   ```

   ```sh
   #!/bin/bash
   for host in hadoop102 hadoop103 hadoop104
   do
   	echo =============== $host ===============
   	ssh $host jps 
   done
   ```

   ```bash
   [lucp@hadoop102 bin]$ chmod 777 jpsall
   ```

   验证上述两步操作

   ```bash
   [lucp@hadoop102 bin]$ ll
   总用量 12
   -rwxrwxrwx  1 lucp lucp  122 4月   3 14:50 jpsall
   -rwxrwxrwx  1 lucp lucp 1022 4月   3 14:47 myhadoop.sh
   -rwxrwxrwx. 1 lucp lucp  737 3月  31 12:42 xsync
   ```

3. 分发`~/bin` 目录，保证自定义脚本在三台机器上都可以使用

   ```bash
   [lucp@hadoop102 bin]$ xsync ~/bin/
   ==================== hadoop102 ====================
   sending incremental file list
   
   sent 137 bytes  received 17 bytes  308.00 bytes/sec
   total size is 1,881  speedup is 12.21
   ==================== hadoop103 ====================
   sending incremental file list
   bin/
   bin/jpsall
   bin/myhadoop.sh
   
   sent 1,370 bytes  received 58 bytes  2,856.00 bytes/sec
   total size is 1,881  speedup is 1.32
   ==================== Hadoop104 ====================
   sending incremental file list
   bin/
   bin/jpsall
   bin/myhadoop.sh
   
   sent 1,370 bytes  received 58 bytes  2,856.00 bytes/sec
   total size is 1,881  speedup is 1.32
   ```

   ```bash
   [lucp@hadoop102 ~]$ myhadoop.sh start
   [lucp@hadoop102 ~]$ myhadoop.sh stop
   ```
   
   ```bash
    =================== 启动 hadoop 集群 ===================
    --------------- 启动 hdfs ---------------
   Starting namenodes on [hadoop102]
   Starting datanodes
   Starting secondary namenodes [hadoop104]
    --------------- 启动 yarn ---------------
   Starting resourcemanager
   Starting nodemanagers
   localhost: nodemanager is running as process 10193.  Stop it first.
    --------------- 启动 historyserver ---------------
   ```
   
   ```bash
   [lucp@hadoop102 ~]$ jpsall
   =============== hadoop102 ===============
   10336 DataNode
   10210 NameNode
   10844 JobHistoryServer
   10925 Jps
   10671 NodeManager
   =============== hadoop103 ===============
   10064 ResourceManager
   10193 NodeManager
   10660 Jps
   9909 DataNode
   =============== hadoop104 ===============
   10256 Jps
   9974 SecondaryNameNode
   10122 NodeManager
   9902 DataNode
   ```
   
   ### 1.1.9 常用端口号说明（重要）
   
   | 端口名称                   | Hadoop2.x   | Hadoop3.x              |
   | -------------------------- | ----------- | ---------------------- |
   | NameNode 内部通信端口      | 8020 / 9000 | **8020** / 9000 / 9820 |
   | NameNode HTTP UI           | 50070       | **9870**               |
   | MapReduce 查看执行任务端口 | 8088        | 8088                   |
   | 历史服务器通信端口         | 19888       | 19888                  |
   
   常用的配置文件 
   
   3.x core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml workers 
   
   2.x core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml slaves

### 1.9 集群时间同步

1. 时间服务器配置

   ```bash
   [lucp@hadoop102 ~]$ sudo systemctl start ntpd
   [lucp@hadoop102 ~]$ sudo systemctl status ntpd
   [lucp@hadoop102 ~]$ sudo systemctl is-enabled ntpd
   ```

   ```bash
   ● ntpd.service - Network Time Service
      Loaded: loaded (/usr/lib/systemd/system/ntpd.service; disabled; vendor preset: disabled)
      Active: active (running) since 一 2023-04-03 15:22:30 CST; 1s ago
     Process: 18520 ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS (code=exited, status=0/SUCCESS)
   ```

## 2. HDFS

### 2.1 HDFS 组成架构

![HDFS 架构](https://hadoop.apache.org/docs/r1.0.4/cn/images/hdfsarchitecture.gif)

1. NameNode（nn）：就是Master，是一个主管、管理者。
   - 管理HDFS的名称空间;
   - 配置副本策略; 
   - 管理数据块(Block)映射信息; 
   - 处理客户端读写请求。 
2. DataNode：就是Slave。下达命令，DataNode执行实际的操作。
   - 存储实际的数据块;
   - 执行数据块的读/写操作。 
3. Client：就是客户端。
   - 文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传；
   - 与NameNode交互，获取文件的位置信息；
   - 与DataNode交互，读取或者写入数据；
   - Client提供一些命令来管理HDFS，比如NameNode格式化；
   - Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作；
4. Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。
   - 辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode ；
   - 在紧急情况下，可辅助恢复NameNode。

### 2.2 HDFS 文件块大小（重点）

HDFS中的文件在物理上是分块存储（Block），块的大小可以通过配置参数( dfs.blocksize）来规定，默认大小在Hadoop2.x/3.x版本中是**128M**，1.x版本中是64M。

为什么块的大小不能设置太小，也不能设置太大 

（1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；

（2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开 始位置所需的时间。导致程序在处理这块数据时，会非常慢  。 

总结：**HDFS块的大小设置主要取决于磁盘传输速率。**

> https://www.cnblogs.com/sunbr/p/13262242.html

### 2.3 HDFS 的 Shell 操作（重点）

```bash
[lucp@hadoop102 hadoop-3.1.3]$ bin/hadoop fs
```

```bash
Usage: hadoop fs [generic options]
        [-appendToFile <localsrc> ... <dst>]
        [-cat [-ignoreCrc] <src> ...]
        [-checksum <src> ...]
        [-chgrp [-R] GROUP PATH...]
        [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
        [-chown [-R] [OWNER][:[GROUP]] PATH...]
        [-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]
        [-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]
        [-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]
        [-createSnapshot <snapshotDir> [<snapshotName>]]
        [-deleteSnapshot <snapshotDir> <snapshotName>]
        [-df [-h] [<path> ...]]
        [-du [-s] [-h] [-v] [-x] <path> ...]
        [-expunge]
        [-find <path> ... <expression> ...]
        [-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-getfacl [-R] <path>]
        [-getfattr [-R] {-n name | -d} [-e en] <path>]
        [-getmerge [-nl] [-skip-empty-file] <src> <localdst>]
        [-head <file>]
        [-help [cmd ...]]
        [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]
        [-mkdir [-p] <path> ...]
        [-moveFromLocal <localsrc> ... <dst>]
        [-moveToLocal <src> <localdst>]
        [-mv <src> ... <dst>]
        [-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]
        [-renameSnapshot <snapshotDir> <oldName> <newName>]
        [-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]
        [-rmdir [--ignore-fail-on-non-empty] <dir> ...]
        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]
        [-setfattr {-n name [-v value] | -x name} <path>]
        [-setrep [-R] [-w] <rep> <path> ...]
        [-stat [format] <path> ...]
        [-tail [-f] [-s <sleep interval>] <file>]
        [-test -[defsz] <path>]
        [-text [-ignoreCrc] <src> ...]
        [-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]
        [-touchz <path> ...]
        [-truncate [-w] <length> <path> ...]
        [-usage [cmd ...]]

Generic options supported are:
-conf <configuration file>        specify an application configuration file
-D <property=value>               define a value for a given property
-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.
-jt <local|resourcemanager:port>  specify a ResourceManager
-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster
-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath
-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines

The general command line syntax is:
command [genericOptions] [commandOptions]
```

常用命令实操

1. 启动 Hadoop 集群

   ```bash
   [lucp@hadoop102 ~]$ myhadoop.sh start
   [lucp@hadoop102 ~]$ jpsall
   ```

2. -help：输出这个命令参数

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -help rm
   -rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ... :
     Delete all files that match the specified file pattern. Equivalent to the Unix
     command "rm <src>"
                                                                                    
     -f          If the file does not exist, do not display a diagnostic message or 
                 modify the exit status to reflect an error.                        
     -[rR]       Recursively deletes directories.                                   
     -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>.  
     -safely     option requires safety confirmation, if enabled, requires          
                 confirmation before deleting large directory with more than        
                 <hadoop.shell.delete.limit.num.files> files. Delay is expected when
                 walking over large directory recursively to count the number of    
                 files to be deleted before the confirmation.
   ```

3. 创建/sanguo 文件夹

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /sanguo
   ```

4. 上传

   1. -moveFromLocal：从本地剪切粘贴到 HDFS

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ vim sanguo.txt
      [lucp@hadoop102 hadoop-3.1.3]$ cat sanguo.txt 
      sanguo
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -moveFromLocal sanguo.txt /sanguo
      ```

   2. -copyFromLocal：从本地文件系统中拷贝文件到 HDFS 路径去

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -copyFromLocal sanguo.txt /sanguo
      ```

   3. -put：等同于 copyFromLocal，生产环境更习惯用 put

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -put test.txt /sanguo
      ```

   4. -appendToFile：追加一个文件到已经存在的文件末尾

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ vim appendFile.txt
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -appendToFile appendFile.txt /sanguo/test.txt
      ```

      ![image-20230404102736431](https://s2.loli.net/2023/04/04/d2tr3Fi4MUpNsve.png)

5. 下载

   1. -copyToLocal：从 HDFS 拷贝到本地

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ rm -f test.txt 
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -copyToLocal /sanguo/test.txt /opt/module/hadoop-3.1.3/
      ```

   2. -get：等同于 copyToLocal，生产环境更习惯用 get

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -get /sanguo/test.txt /opt/module/hadoop-3.1.3/
      ```

6. HDFS 直接操作

   1. -ls: 显示目录信息

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /sanguo
      Found 2 items
      -rw-r--r--   3 lucp supergroup          7 2023-04-04 10:17 /sanguo/sanguo.txt
      -rw-r--r--   3 lucp supergroup         21 2023-04-04 10:25 /sanguo/test.txt
      ```

   2. -cat：显示文件内容

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -cat /sanguo/test.txt
      2023-04-04 10:34:52,411 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
      test -put
      appendFile
      ```

   3. -chgrp、-chmod、-chown：Linux 文件系统中的用法一样，修改文件所属权限

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -chmod 666 /sanguo/test.txt
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -ls /sanguo
      Found 2 items
      -rw-r--r--   3 lucp supergroup          7 2023-04-04 10:17 /sanguo/sanguo.txt
      -rw-rw-rw-   3 lucp supergroup         21 2023-04-04 10:25 /sanguo/test.txt
      ```

   4. -mkdir：创建路径

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -mkdir /shuihu
      ```

   5. -cp：从 HDFS 的一个路径拷贝到 HDFS 的另一个路径

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -cp /sanguo/sanguo.txt /shuihu
      ```

   6. -mv：在 HDFS 目录中移动文件

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -mv /sanguo/test.txt /shuihu
      ```

   7. -tail：显示一个文件的末尾 1kb 的数据

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -tail /sanguo/sanguo.txt
      ```

   8. -rm：删除文件或文件夹

   9. -rm -r：递归删除目录及目录里面内容

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -rm -r /jinguo
      ```

   10. -du 统计文件夹的大小信息

       ```bash
       [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -du -s -h /sanguo
       7  21  /sanguo
       [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -du -h /shuihu
       7   21  /shuihu/sanguo.txt
       21  63  /shuihu/test.txt
       ```

   11. -setrep：设置 HDFS 中文件的副本数量

       ```bash
       [lucp@hadoop102 hadoop-3.1.3]$ hadoop fs -setrep 10 /shuihu/test.txt
       Replication 10 set: /shuihu/test.txt
       ```

       ![image-20230404111110327](https://s2.loli.net/2023/04/04/54mwotKPbxAYUOT.png)

       > 这里设置的副本数只是记录在 NameNode 的元数据中，是否真的会有这么多副本，还得 看 DataNode 的数量。因为目前只有 3 台设备，最多也就 3 个副本，只有节点数的增加到 10 台时，副本数才能达到 10。

### 2.4 HDFS 的 API 操作

1. 环境配置

![image-20230404111846438](https://s2.loli.net/2023/04/04/JFud9MzLnCZIg1A.png)

![image-20230404112116076](https://s2.loli.net/2023/04/04/1nz5RsT2umHP6be.png)

> 验证 Hadoop 环境变量是否正常。双击 A:\Env\hadoop-3.1.0\bin\winutils.exe(环境路径)



> 需要检查的配置 `A:\Env\maven\apache-maven-3.6.3\conf\settings.xml`

```xml
55  <localRepository>A:\Env\maven\mvn-repository</localRepository>

159 <mirror>
    <id>alimaven</id>
    <mirrorOf>central</mirrorOf>
    <name>aliyun maven</name>
    <url>https://maven.aliyun.com/mvn/view</url>
</mirror>

189 <profile>
	<id>jdk-1.8</id>
	<activation>
	<activeByDefault>true</activeByDefault>
	<jdk>1.8</jdk>
	</activation>
	<properties>
	<maven.compiler.source>1.8</maven.compiler.source>
	<maven.compiler.target>1.8</maven.compiler.target>
	<maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion>
	</properties>
</profile>
```

![image-20230406090845059](https://s2.loli.net/2023/04/06/4Tg3VqdhrklCNmt.png)

![image-20230411152008205](https://s2.loli.net/2023/04/11/2XaKxHg3VM6mZ7L.png)

```bash
C:\Users\Lucp>mvn -v
Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)
Maven home: A:\Env\maven\apache-maven-3.6.3\bin\..
Java version: 1.8.0_362, vendor: Amazon.com Inc., runtime: A:\Env\java\jdk8\jre
Default locale: zh_CN, platform encoding: GBK
OS name: "windows 10", version: "10.0", arch: "amd64", family: "windows"
```



2. 在 IDEA 中创建一个 Maven 工程 HdfsClientDemo，并导入相应的依赖坐标+日志添加

   ![image-20230404133543810](https://s2.loli.net/2023/04/04/T7XcHit3BLzqNsI.png)

   ```xml
   <dependencies>
    <dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>3.1.3</version>
    </dependency>
    <dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <version>4.12</version>
    </dependency>
    <dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-log4j12</artifactId>
    <version>1.7.30</version>
    </dependency>
   </dependencies>
   ```

3. 在项目的 src/main/resources 目录下，新建一个文件，命名为“log4j.properties”，在文件中填入

   ```properties
   log4j.rootLogger=INFO, stdout
   log4j.appender.stdout=org.apache.log4j.ConsoleAppender
   log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
   log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
   log4j.appender.logfile=org.apache.log4j.FileAppender
   log4j.appender.logfile.File=target/spring.log
   log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
   log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
   ```

4. 创建包名：com.lucp.hdfs

   ![image-20230404134455448](https://s2.loli.net/2023/04/04/fgQIFzRUcMbitWu.png)

5. 创建 HdfsClient 类

   ![image-20230404134756259](https://s2.loli.net/2023/04/04/yrMaZH9SRLEN423.png)

6. 自动导入

   ![image-20230404135251466](https://s2.loli.net/2023/04/04/gJYhBIoGMwTdOkA.png)



### 2.5 HDFS 的 API 案例实操

> [Hadoop API](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/filesystem/filesystem.html#boolean_rename.28Path_src.2C_Path_d.29)

1. 创建文件夹

   ```java
   package com.lucp.hdfs;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.FileSystem;
   import org.apache.hadoop.fs.Path;
   import org.junit.Test;
   
   import java.io.IOException;
   import java.net.URI;
   import java.net.URISyntaxException;
   
   public class HdfsClient {
       @Test
       public void testMkdir() throws URISyntaxException, IOException, InterruptedException {
           // 获取集群地址
           URI uri = new URI("hdfs://hadoop102:8020");
           // 获取配置信息
           Configuration configuration = new Configuration();
           // 用户信息
           String user = "lucp";
           // 获取客户端
           FileSystem fs = FileSystem.get(uri,configuration,user);
   
           // 相关操作——创建文件夹
           fs.mkdirs(new Path("/xiyouji"));
   
           // 关闭资源
           fs.close();
       }
   }
   ```

   ```java
   package com.lucp.hdfs;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.FileSystem;
   import org.apache.hadoop.fs.Path;
   import org.junit.After;
   import org.junit.Before;
   import org.junit.Test;
   
   import java.io.IOException;
   import java.net.URI;
   import java.net.URISyntaxException;
   
   public class HdfsClient {
       private FileSystem fs;
       
       @Before
       public void init() throws URISyntaxException, IOException, InterruptedException {
           // 获取集群地址
           URI uri = new URI("hdfs://hadoop102:8020");
           // 获取配置信息
           Configuration configuration = new Configuration();
           // 用户信息
           String user = "lucp";
           // 获取客户端
           fs = FileSystem.get(uri,configuration,user);
       }
   
       @After
       public void close() throws IOException {
           // 关闭资源
           fs.close();
       }
       
       @Test
       public void testMkdir() throws  IOException {
   
           // 相关操作——创建文件夹
           fs.mkdirs(new Path("/xiyouji"));
       }
   }
   ```

2. 文件上传

   ```java
   @Test
   public void put() throws IOException {
   	fs.copyFromLocalFile(false,false,new Path("C:\\Users\\Lucp\\Desktop\\sunwukong.txt"), new Path("/xiyouji"));
       // copyFromLocalFile 删除原数据，是否覆盖（不可追加），原始路径，上传的目的地址
   }
   ```

   参数优先级：

   resources 资源目录下的 hdfs-site.xml 文件

   ```xml
   <?xml version="1.0" encoding="UTF-8"?>
   <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
   <configuration>
       <property>
           <name>dfs.replication</name>
           <value>1</value>
       </property>
   </configuration>
   ```

   ![image-20230406140339965](https://s2.loli.net/2023/04/06/fLv315UpsMBuk62.png)

   ```java
   @Before
   public void init() throws URISyntaxException, IOException, InterruptedException {
       // 获取集群地址
       URI uri = new URI("hdfs://hadoop102:8020");
       // 获取配置信息
       Configuration configuration = new Configuration();
       
       // 测试参数优先级
       configuration.set("dfs.replication", "2");
       
       // 用户信息
       String user = "lucp";
       // 获取客户端
       fs = FileSystem.get(uri,configuration,user);
   }
   ```

   ![image-20230406140536714](https://s2.loli.net/2023/04/06/lFI32NGZsbhxd1i.png)

   > 参数优先级排序：（1）客户端代码中设置的值 >（2）ClassPath 下的用户自定义配置文 件 >（3）然后是服务器的自定义配置（xxx-site.xml）>（4）服务器的默认配置（xxx-default.xml）

3. 文件下载

   ```java
   @Test
   public void get() throws IOException {
       fs.copyToLocalFile(false,new Path("/xiyouji/sunwukong.txt"),new Path("C:\\Users\\Lucp\\Desktop"),true);
       /* copyToLocalFile参数：
       boolean delSrc 指是否将原文件删除
       Path src 指要下载的文件路径
       Path dst 指将文件下载到的路径
       boolean useRawLocalFileSystem 是否开启文件校验，默认开启(false)
       */
   }
   ```

4. 文件更名和移动

   ```java
   @Test
   public void move() throws IOException {
       /*
       Path src 原始文件路径
       Path d 目的文件路径
        */
       // 更名
       fs.rename(new Path("/xiyouji/sunwukong1.txt"),new Path("/xiyouji/sunwukong.txt"));
       // 移动
       fs.rename(new Path("/xiyouji/sunwukong.txt"),new Path("/shuihu/sunwukong1.txt"));
   }
   ```

   ![image-20230406144656060](https://s2.loli.net/2023/04/06/K3EtgSZWGxUHkqP.png)

5. 删除文件和目录

   ```java
   @Test
   public void del() throws IOException {
       fs.delete(new Path("/sanguo/shuguo"),false);
       // boolean delete(Path p, boolean recursive)
       // Path ：所删除文件的路径，recursive ：true删除非空目录；false删除空目录
   }
   ```

6. 文件详情查看

   ```java
   @Test
   public void getFileInfo() throws IOException {
       // LocatedFileStatus listfile = fs.listFiles(new Path(""),true).next();
       RemoteIterator<LocatedFileStatus> listFile = fs.listFiles(new Path("/sanguo"),true);
       // listFiles(Path path, boolean recursive)
       while (listFile.hasNext()){
           LocatedFileStatus fileStatus = listFile.next();
           System.out.println("========" + fileStatus.getPath() + "=========");
           System.out.println(fileStatus.getPermission());
           System.out.println(fileStatus.getOwner());
           System.out.println(fileStatus.getGroup());
           System.out.println(fileStatus.getLen());
           System.out.println(fileStatus.getModificationTime());
           System.out.println(fileStatus.getReplication());
           System.out.println(fileStatus.getBlockSize());
           System.out.println(fileStatus.getPath().getName());
   
           // 获取块信息
           BlockLocation[] blockLocations = fileStatus.getBlockLocations();
           System.out.println(Arrays.toString(blockLocations));
       }
   }
   ```

   ```bash
   ========hdfs://hadoop102:8020/sanguo/sanguo.txt=========
   rw-r--r--
   lucp
   supergroup
   7
   1680574652336
   3
   134217728
   sanguo.txt
   [0,7,hadoop104,hadoop102,hadoop103]
   ========hdfs://hadoop102:8020/sanguo/weiguo/caocao.txt=========
   rw-r--r--
   lucp
   supergroup
   6
   1680759814244
   3
   134217728
   caocao.txt
   [0,6,hadoop104,hadoop102,hadoop103]
   ```

7. 文件和文件夹判断

   ```java
   @Test
   public void isFile() throws IOException {
       FileStatus[] listStatus = fs.listStatus(new Path("/sanguo"));
       for (FileStatus fileStatus : listStatus) {
           // 如果是文件
           if (fileStatus.isFile()) {
               System.out.println("f:"+fileStatus.getPath().getName());
           }else {
               System.out.println("d:"+fileStatus.getPath().getName());
           }
       }
   }
   ```

   ```bash
   f:sanguo.txt
   d:weiguo
   ```

   例子备份：

   ```java
   package com.lucp.hdfs;
   
   import org.apache.hadoop.conf.Configuration;
   import org.apache.hadoop.fs.*;
   import org.junit.After;
   import org.junit.Before;
   import org.junit.Test;
   
   import java.io.IOException;
   import java.net.URI;
   import java.net.URISyntaxException;
   import java.util.Arrays;
   
   public class HdfsClient {
       private FileSystem fs;
       @Before
       public void init() throws URISyntaxException, IOException, InterruptedException {
           // 获取集群地址
           URI uri = new URI("hdfs://hadoop102:8020");
           // 获取配置信息
           Configuration configuration = new Configuration();
           // 用户信息
           String user = "lucp";
           // 获取客户端
           fs = FileSystem.get(uri,configuration,user);
       }
   
       @After
       public void close() throws IOException {
           // 关闭资源
           fs.close();
       }
   
       @Test
       public void testMkdir() throws  IOException {
           // 相关操作——创建文件夹
           fs.mkdirs(new Path("/xiyouji"));
       }
       @Test
       public void put() throws IOException {
           fs.copyFromLocalFile(false,true,new Path("C:\\Users\\Lucp\\Desktop\\sunwukong.txt"), new Path("/xiyouji"));
           // copyFromLocalFile 删除原数据，是否覆盖（不可追加），原始路径，上传的目的地址
       }
   
       @Test
       public void get() throws IOException {
           fs.copyToLocalFile(false,new Path("/xiyouji/sunwukong.txt"),new Path("C:\\Users\\Lucp\\Desktop"),true);
       /* copyToLocalFile参数：
       boolean delSrc 指是否将原文件删除
       Path src 指要下载的文件路径
       Path dst 指将文件下载到的路径
       boolean useRawLocalFileSystem 是否开启文件校验，默认开启
       */
       }
   
       @Test
       public void move() throws IOException {
           /*
           Path src 原始文件路径
           Path d 目的文件路径
            */
           // 更名
           fs.rename(new Path("/xiyouji/sunwukong1.txt"),new Path("/xiyouji/sunwukong.txt"));
           // 移动
           fs.rename(new Path("/xiyouji/sunwukong.txt"),new Path("/shuihu/sunwukong1.txt"));
       }
   
       @Test
       public void del() throws IOException {
           fs.delete(new Path("/sanguo/shuguo"),false);
           // boolean delete(Path p, boolean recursive)
           // Path ：所删除文件的路径，recursive ：true删除非空目录；false删除空目录
       }
   
       @Test
       public void getFileInfo() throws IOException {
           // LocatedFileStatus listfile = fs.listFiles(new Path(""),true).next();
           RemoteIterator<LocatedFileStatus> listFile = fs.listFiles(new Path("/sanguo"),true);
           // listFiles(Path path, boolean recursive)
           while (listFile.hasNext()){
               LocatedFileStatus fileStatus = listFile.next();
               System.out.println("========" + fileStatus.getPath() + "=========");
               System.out.println(fileStatus.getPermission());
               System.out.println(fileStatus.getOwner());
               System.out.println(fileStatus.getGroup());
               System.out.println(fileStatus.getLen());
               System.out.println(fileStatus.getModificationTime());
               System.out.println(fileStatus.getReplication());
               System.out.println(fileStatus.getBlockSize());
               System.out.println(fileStatus.getPath().getName());
   
               // 获取块信息
               BlockLocation[] blockLocations = fileStatus.getBlockLocations();
               System.out.println(Arrays.toString(blockLocations));
           }
       }
   
       @Test
       public void isFile() throws IOException {
           FileStatus[] listStatus = fs.listStatus(new Path("/sanguo"));
           for (FileStatus fileStatus : listStatus) {
               // 如果是文件
               if (fileStatus.isFile()) {
                   System.out.println("f:"+fileStatus.getPath().getName());
               }else {
                   System.out.println("d:"+fileStatus.getPath().getName());
               }
           }
       }
   }
   ```


### 2.6 HDFS 的读写流程（面试重点）

1. HDFS 写数据流程

   <img src="https://s2.loli.net/2023/04/07/w8532Hdt7zNqcve.png" alt="image-20230407091115241" style="zoom:80%;" />

   1. 客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。
   2. NameNode 返回是否可以上传。
   3. 客户端请求第一个 Block 上传到哪几个 DataNode 服务器上。
   4. NameNode 返回 3 个 DataNode 节点，分别为 dn1、dn2、dn3。
   5. 客户端通过 FSDataOutputStream 模块请求 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。
   6. dn1、dn2、dn3 逐级应答客户端。
   7. 客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存）， 以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个 packet 会放入一个应答队列等待应答。
   8. 当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务 器。（重复执行 3-7 步）。

2. 网络拓扑-节点距离计算

   1. 节点距离：两个节点到达最近的共同祖先的距离总和。<img src="https://s2.loli.net/2023/04/07/zuoLY64OjqbKEkR.png" alt="image-20230407092402037" style="zoom:67%;" />

3. 机架感知（副本存储节点选择）

   http://hadoop.apache.org/docs/r3.1.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication

4. 副本节点选择<img src="https://s2.loli.net/2023/04/07/lXj1SZHDAzRK3gu.png" alt="image-20230407093900494" style="zoom: 67%;" />

5. HDFS 读数据流程

   <img src="https://s2.loli.net/2023/04/07/c7DGPIOwLzo6RTx.png" alt="image-20230407094718508" style="zoom:80%;" />

   1. 客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。
   2. 挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。
   3. DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）。
   4. 客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件

### 2.7 NameNode 和 SecondaryNameNode

1. NN 和 2NN 工作机制![bd_NameNode工作机制.png](https://s2.loli.net/2023/04/07/5ESZF4idDW7zLbP.png)
   1. 第一阶段：NameNode 启动
      1. 第一次启动 NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。
      2. 客户端对元数据进行增删改的请求。
      3. NameNode 记录操作日志，更新滚动日志。
      4. NameNode 在内存中对元数据进行增删改。
   2. 第二阶段：Secondary NameNode 工作 
      1. Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode 是否检查结果。 
      2. Secondary NameNode 请求执行 CheckPoint。
      3. NameNode 滚动正在写的 Edits 日志。
      4. 将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。
      5. Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。
      6. 生成新的镜像文件 fsimage.chkpoint。
      7. 拷贝 fsimage.chkpoint 到 NameNode。
      8. NameNode 将 fsimage.chkpoint 重新命名成 fsimage。

### 2.8 DataNode 工作机制

![image-20230407104023154](https://s2.loli.net/2023/04/07/AUki2fFOXqbCJKt.png)

1. dddd

   1. 一个数据块在 DataNode 上以文件形式存储在磁盘上，包括两个文件，一个是数据 本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。 

   2. DataNode 启动后向 NameNode 注册，通过后，周期性（6 小时）的向 NameNode 上 报所有的块信息。DN 向 NN 汇报当前解读信息的时间间隔，默认 6 小时；

      ```xml
      <property>
      <name>dfs.blockreport.intervalMsec</name>
      <value>21600000</value>
      <description>Determines block reporting interval in 
      milliseconds.</description>
      </property>
      ```

      ```xml
      <property>
      <name>dfs.datanode.directoryscan.interval</name>
      <value>21600s</value>
      <description>Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.
      	Support multiple time unit suffix(case insensitive), as described in dfs.heartbeat.interval.
      </description>
      </property>
      ```

   3. 心跳是每 3 秒一次，心跳返回结果带有 NameNode 给该 DataNode 的命令如复制块 数据到另一台机器，或删除某个数据块。如果超过 10 分钟没有收到某个 DataNode 的心跳， 则认为该节点不可用。

   4. 集群运行中可以安全加入和退出一些机器。

2. 数据完整性

   1. 如下是 DataNode 节点保证数据完整性的方法。
      1. 当 DataNode 读取 Block 的时候，它会计算 CheckSum。 
      2. 如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。
      3. Client 读取其他 DataNode 上的 Block。
      4. 常见的校验算法 crc（32），md5（128），sha1（160）
      5. DataNode 在其文件创建后周期验证 CheckSum。

## 3. Mapreduce

### 3.1 概述

1. MapReduce 核心思想![image-20230407142401221](https://s2.loli.net/2023/04/07/w2hnGZcCQXTDuoy.png)

2. MapReduce 进程

   一个完整的 MapReduce 程序在分布式运行时有三类实例进程：

   1. MrAppMaster：负责整个程序的过程调度及状态协调。
   2. MapTask：负责 Map 阶段的整个数据处理流程。
   3. ReduceTask：负责 Reduce 阶段的整个数据处理流程。

3. 官方 WordCount 源码

4. 常用数据序列化类型

5. MapReduce 编程规范

   用户编写的程序分成三个部分：Mapper、Reducer 和 Driver。

   1. Mapper 阶段

      1. 用户自定义的Mapper要继承自己的父类
      2. Mapper的输入数据是KV对的形式（KV的类型可自定义）
      3. Mapper中的业务逻辑写在map()方法中
      4. Mapper的输出数据是KV对的形式（KV的类型可自定义 ）
      5. map()方法（MapTask进程）对每一个调用一次

   2. Reducer阶段

      1. 用户自定义的Reducer要继承自己的父类 
      2. Reducer的输入数据类型对应Mapper的输出数据类型，也是KV 
      3. Reducer的业务逻辑写在reduce()方法中 
      4. ReduceTask进程对每一组相同k的<k,v>组调用一次reduce()方法 

   3. Driver阶段

      相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是 封装了MapReduce程序相关运行参数的job对象

6. WordCount 案例实操

   1. 本地测试

      1. 需求分析![image-20230407144052509](https://s2.loli.net/2023/04/07/SFLnD8dHICoEV4x.png)

      2. 环境准备

         1. 创建 maven 工程，MapReduceDemo，更改maven配置

         2. 在 pom.xml 文件中添加如下依赖

            ```xml
            <dependencies>
                <dependency>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>hadoop-client</artifactId>
                    <version>3.1.3</version>
                </dependency>
                <dependency>
                    <groupId>junit</groupId>
                    <artifactId>junit</artifactId>
                    <version>4.12</version>
                </dependency>
                <dependency>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                    <version>1.7.30</version>
                </dependency>
            </dependencies>
            ```

         3. 在项目的 src/main/resources 目录下，新建一个文件，命名为“log4j.properties”，在 文件中填入。

            ```properties
            log4j.rootLogger=INFO, stdout
            log4j.appender.stdout=org.apache.log4j.ConsoleAppender
            log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
            log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
            log4j.appender.logfile=org.apache.log4j.FileAppender
            log4j.appender.logfile.File=target/spring.log
            log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
            log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
            ```

         4. 创建包名：com.lucp.mapreduce.wordcount

      3. 编写程序

         1. 编写 Mapper 类  `WordCountMapper`

            ```java
            package com.lucp.mapreduce.wordcount;
            
            import org.apache.hadoop.io.IntWritable;
            import org.apache.hadoop.io.LongWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Mapper;
            
            import java.io.IOException;
            
            // 参数1：map阶段输入的key类型（LongWritable）
            // 参数2：map阶段输入的value类型（Text）
            // 参数3：map阶段输出的key类型（Text）
            // 参数4：map阶段输出的value类型（IntWritable）
            
            public class WordCountMapper extends Mapper<LongWritable,Text,Text,IntWritable> {
            
                private Text k = new Text();
                private IntWritable v = new IntWritable(1);
            
                @Override
                protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context) throws IOException, InterruptedException {
                    // 1 获取一行
                    String line = value.toString();
                    // 2 切割
                    String[] words = line.split(" ");
                    // 3 输出
                    for (String word : words) {
                        k.set(word);
                        context.write(k, v);
                    }
                }
            }
            ```

         2. 编写 Reducer 类 `WordCountReducer`

            ```java
            package com.lucp.mapreduce.wordcount;
            import java.io.IOException;
            import org.apache.hadoop.io.IntWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Reducer;
            public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
                int sum;
                IntWritable v = new IntWritable();
                @Override
                protected void reduce(Text key, Iterable<IntWritable> values,Context
                        context) throws IOException, InterruptedException {
                    // 1 累加求和
                    sum = 0;
                    for (IntWritable count : values) {
                        sum += count.get();
                    }
                    // 2 输出
                    v.set(sum);
                    context.write(key,v);
                }
            }
            ```

         3. 编写 Driver 驱动类 `WordCountDriver`

            ```java
            package com.lucp.mapreduce.wordcount;
            
            import org.apache.hadoop.conf.Configuration;
            import org.apache.hadoop.fs.Path;
            import org.apache.hadoop.io.IntWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Job;
            import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
            import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
            
            import java.io.IOException;
            
            public class WordCountDriver {
            
                public static void main (String[] args) throws IOException, InterruptedException, ClassNotFoundException {
                    // 1. 获取配置信息以及获取 job 对象
                    Configuration conf = new Configuration();
                    Job job = Job.getInstance(conf);
            
                    // 2. 获取Jar包所在路径
                    job.setJarByClass(WordCountDriver.class);
            
                    // 3. 关联mapper和reducer
                    job.setMapperClass(WordCountMapper.class);
                    job.setReducerClass(WordCountReducer.class);
            
                    // 4. 指定map输出的kv类型
                    job.setMapOutputKeyClass(Text.class);
                    job.setMapOutputValueClass(IntWritable.class);
            
                    // 5. 指定最终输出的kv类型
                    job.setOutputKeyClass(Text.class);
                    job.setOutputValueClass(IntWritable.class);
            
                    // 6. 指定输入输出路径
                    FileInputFormat.setInputPaths(job,new Path("C:\\Users\\Lucp\\Desktop\\dsj\\inputword"));
                    FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\outword"));
            
                    // 7. 提交作业
                    boolean result = job.waitForCompletion(true);
                }
            }
            ```

            > 运行前需新建文件夹`C:\Users\Lucp\Desktop\dsj\inputword`，内含一txt文件
            >
            > 运行前需使输出路径`C:\Users\Lucp\Desktop\dsj\outword`不存在

   2. 提交到集群测试

      1. 在pom.xml添加依赖

         ```xml
         <build>
             <plugins>
                 <plugin>
                     <artifactId>maven-compiler-plugin</artifactId>
                     <version>3.6.1</version>
                     <configuration>
                         <source>1.8</source>
                         <target>1.8</target>
                     </configuration>
                 </plugin>
                 <plugin>
                     <artifactId>maven-assembly-plugin</artifactId>
                     <configuration>
                         <descriptorRefs>
                             <descriptorRef>jar-with-dependencies</descriptorRef>
                         </descriptorRefs>
                     </configuration>
                     <executions>
                         <execution>
                             <id>make-assembly</id>
                             <phase>package</phase>
                             <goals>
                                 <goal>single</goal>
                             </goals>
                         </execution>
                     </executions>
                 </plugin>
             </plugins>
         </build>
         ```

      2. 将程序打成jar包![image-20230410112746150](https://s2.loli.net/2023/04/10/g6UiY5sv9qdoXnB.png)

      3. 修改不带依赖的 jar 包名称为 mr.jar，并拷贝该 jar 包到 Hadoop 集群的 /opt/module/hadoop-3.1.3 路径。![image-20230410114111296](https://s2.loli.net/2023/04/10/9UfpAdqzakiWEbt.png)

      4. 启动集群

         ```bash
         [lucp@hadoop102 ~]$ myhadoop.sh start
         ```

      5. 执行WordCount程序

         ```bash
         [lucp@hadoop102 hadoop-3.1.3]$ hadoop jar mr.jar com/lucp/mapreduce/wordcount/WordCountDriver /wcinput /wcoutput
         ```

         > `com/lucp/mapreduce/wordcount/WordCountDriver`来源![image-20230410115038427](https://s2.loli.net/2023/04/10/JicOTutfp1Mbra4.png)

         ```bash
         2023-04-10 11:37:53,252 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032
         2023-04-10 11:37:54,236 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
         2023-04-10 11:37:54,447 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/lucp/.staging/job_1681096500861_0001
         2023-04-10 11:37:54,569 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
         2023-04-10 11:37:55,687 INFO input.FileInputFormat: Total input files to process : 1
         2023-04-10 11:37:55,774 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
         2023-04-10 11:37:55,809 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
         2023-04-10 11:37:55,843 INFO mapreduce.JobSubmitter: number of splits:1
         2023-04-10 11:37:56,008 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
         2023-04-10 11:37:56,077 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1681096500861_0001
         2023-04-10 11:37:56,077 INFO mapreduce.JobSubmitter: Executing with tokens: []
         2023-04-10 11:37:56,607 INFO conf.Configuration: resource-types.xml not found
         2023-04-10 11:37:56,607 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
         2023-04-10 11:37:57,457 INFO impl.YarnClientImpl: Submitted application application_1681096500861_0001
         2023-04-10 11:37:57,644 INFO mapreduce.Job: The url to track the job: http://hadoop103:8088/proxy/application_1681096500861_0001/
         2023-04-10 11:37:57,646 INFO mapreduce.Job: Running job: job_1681096500861_0001
         2023-04-10 11:38:08,201 INFO mapreduce.Job: Job job_1681096500861_0001 running in uber mode : false
         2023-04-10 11:38:08,202 INFO mapreduce.Job:  map 0% reduce 0%
         2023-04-10 11:38:16,700 INFO mapreduce.Job:  map 100% reduce 0%
         2023-04-10 11:38:21,791 INFO mapreduce.Job:  map 100% reduce 100%
         2023-04-10 11:38:21,808 INFO mapreduce.Job: Job job_1681096500861_0001 completed successfully
         2023-04-10 11:38:21,918 INFO mapreduce.Job: Counters: 53
         ```

         ![image-20230410114602237](https://s2.loli.net/2023/04/10/Kikludowf6PngJt.png)

### 3.2 序列化

1. 什么是序列化

   1. 序列化就是**把内存中的对象，转换成字节序列**（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。 
   2. 反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。

2. 序列化案例实操

   1. 编写 MapReduce 程序

      1. 编写流量统计的 Bean 对象  `FlowBean`

         ```java
         package com.lucp.mapreduce.writable;
         
         import org.apache.hadoop.io.Writable;
         
         import java.io.DataInput;
         import java.io.DataOutput;
         import java.io.IOException;
         
         //1. 继承 Writable 接口
         public class FlowBean implements Writable {
         
             private long upflow;    // 上行流量
             private long downflow;  // 下行流量
             private long sumflow;   // 总流量
         
             // 2. 空参构造函数
             public FlowBean() {
         
             }
         
             // 3. 提供三个参数的 getter 和 setter 方法
             // 鼠标右键 -> 生成 -> Getter and Setter
             public long getUpflow() {
                 return upflow;
             }
         
             public void setUpflow(long upflow) {
                 this.upflow = upflow;
             }
         
             public long getDownflow() {
                 return downflow;
             }
         
             public void setDownflow(long downflow) {
                 this.downflow = downflow;
             }
         
             public long getSumflow() {
                 return sumflow;
             }
         
             public void setSumflow(long sumflow) {
                 this.sumflow = sumflow;
             }
         
             // 函数重载
             public void setSumflow(){
                 this.sumflow = this.upflow + this.downflow;
             }
         
             // 4. 实现序列化和反序列化方法,注意顺序一定要保持一致
             // 重写序列化方法
             @Override
             public void write(DataOutput dataOutput) throws IOException {
                 dataOutput.writeLong(upflow);
                 dataOutput.writeLong(downflow);
                 dataOutput.writeLong(sumflow);
             }
         
             // 重写反序列化方法
             @Override
             public void readFields(DataInput dataInput) throws IOException {
                 this.upflow = dataInput.readLong();
                 this.downflow = dataInput.readLong();
                 this.sumflow = dataInput.readLong();
             }
         
             // 5. 重写 ToString
             @Override
             public String toString() {
                 return upflow + "\t" + downflow + "\t" + sumflow;
             }
         }
         ```
   
      2. 编写 Mapper 类 `mapWritable`
   
         ```java
         package com.lucp.mapreduce.writable;
         
         import org.apache.hadoop.io.LongWritable;
         import org.apache.hadoop.io.Text;
         import org.apache.hadoop.mapreduce.Mapper;
         
         import java.io.IOException;
         
         public class mapWritable extends Mapper<LongWritable, Text,Text,FlowBean> {
         
             private Text outK = new Text();
             private FlowBean outV = new FlowBean();
         
             @Override
             protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
         
                 // 1. 获取一行数据
                 String line = value.toString();
         
                 // 2. 切割
                 String[] split = line.split("\t");
         
                 // 3. 需要获取手机号以及上下行流量
                 String phone = split[1];
                 String up = split[split.length - 3];
                 String down = split[split.length - 2];
         
                 // 4. 封装 outK outV
                 outK.set(phone);
                 outV.setUpflow(Long.parseLong(up));
                 outV.setDownflow(Long.parseLong(down));
                 outV.setSumflow();
         
                 // 5.写出 outK outV
                 context.write(outK, outV);
         
             }
         }
         ```
   
      3. 编写 Reducer 类 `reduceWritable`
   
         ```java
         package com.lucp.mapreduce.writable;
         
         import org.apache.hadoop.io.Text;
         import org.apache.hadoop.mapreduce.Reducer;
         
         import java.io.IOException;
         
         public class reduceWritable extends Reducer<Text,FlowBean,Text,FlowBean> {
             private  FlowBean outV = new FlowBean();
         
             @Override
             protected void reduce(Text key, Iterable<FlowBean> values, Reducer<Text, FlowBean, Text, FlowBean>.Context context) throws IOException, InterruptedException {
         
                 // 遍历循环累加上下行流量
                 long totalup = 0;
                 long totaldown = 0;
                 for (FlowBean value : values){
                     totalup += value.getUpflow();
                     totaldown += value.getDownflow();
                 }
                 outV.setUpflow(totalup);
                 outV.setDownflow(totaldown);
                 outV.setSumflow();
         
                 // 写出
                 context.write(key,outV);
             }
         }
         ```
   
      4. 编写 Driver 驱动类 `driveWritable`
   
         ```java
         package com.lucp.mapreduce.writable;
         
         import org.apache.hadoop.conf.Configuration;
         import org.apache.hadoop.fs.Path;
         import org.apache.hadoop.io.Text;
         import org.apache.hadoop.mapreduce.Job;
         import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
         import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
         
         import java.io.IOException;
         
         public class driveWritable {
             public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
         
                 // 1. 获取 job 对象
                 Configuration conf = new Configuration();
                 Job job = Job.getInstance(conf);
         
                 // 2. 关联本 Driver 类
                 job.setJarByClass(driveWritable.class);
         
                 //3. 关联 Mapper 和 Reducer
                 job.setMapperClass(mapWritable.class);
                 job.setReducerClass(reduceWritable.class);
         
                 // 4. 设置 Map 端输出 KV 类型
                 job.setMapOutputKeyClass(Text.class);
                 job.setMapOutputValueClass(FlowBean.class);
         
                 // 5. 设置程序最终输出的 KV 类型
                 job.setOutputKeyClass(Text.class);
                 job.setOutputValueClass(FlowBean.class);
         
                 // 6. 设置程序的输入输出路径
                 FileInputFormat.setInputPaths(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\inputWritable"));
                 FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\outWritable"));
         
                 // 7. 提交 Job
                 boolean b = job.waitForCompletion(true);
                 System.exit(b ? 0 : 1);
             }
         }
         ```

### 3.3 核心框架原理

1. InputFormat 数据输入
   1. 切片与 MapTask 并行度决定机制

   2. Job 提交流程源码和切片源码详解

      1. 计算切片大小 `computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M`
      2. 每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片

   3. FileInputFormat 切片机制

   4. TextInputFormat 

      1. FileInputFormat 实现类
      2. TextInputFormat

   5. CombineTextInputFormat 切片机制

      ![image-20230411102005240](https://s2.loli.net/2023/04/11/zcUgwlhT4kneDQB.png)

      当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时 将文件均分成 2 个虚拟存储块（防止出现太小切片）。

      例如 setMaxInputSplitSize 值为 4M，输入文件大小为 8.02M，则先逻辑上分成一个 4M。剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储 文件，所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件。

   6. CombineTextInputFormat 案例实操

      1. 将输入的大量小文件合并成一个切片统一处理。

      2. 不做任何处理，运行 1.8 节的 WordCount 案例程序，观察切片个数为 4。

         ```bash
         2023-04-11 10:44:35,703 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:4
         ```

      3. 在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 3。 驱动类中添加代码如下：

         ```java
         // 如果不设置 InputFormat，它默认用的是 TextInputFormat.class
         job.setInputFormatClass(CombineTextInputFormat.class);
         //虚拟存储切片最大值设置 4m
         CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);
         ```

         ```bash
         2023-04-11 10:46:56,307 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:3
         ```

      4. 在 WordcountDriver 中增加如下代码，运行程序，并观察运行的切片个数为 1。

         ```java
         // 如果不设置 InputFormat，它默认用的是 TextInputFormat.class
         job.setInputFormatClass(CombineTextInputFormat.class);
         //虚拟存储切片最大值设置 20m
         CombineTextInputFormat.setMaxInputSplitSize(job, 20971520);
         ```

         ```bash
         2023-04-11 11:02:24,620 INFO [org.apache.hadoop.mapreduce.JobSubmitter] - number of splits:1
         ```

2. MapReduce 工作流程

   ![image-20230411110750166](https://s2.loli.net/2023/04/11/vsaZ75AO9rUo4Cp.png)![image-20230411111606792](https://s2.loli.net/2023/04/11/tRCQ2SFh3yAr8jO.png)

3. Shuffle 机制

   1. Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。![image-20230411112606937](https://s2.loli.net/2023/04/11/uDRQ1dfgHKZyJFC.png)

   2. Partition 分区

      1. 默认Partitioner分区

         ```java
         public class HashPartitioner<K, V> extends Partitioner<K, V> {
         	public int getPartition(K key, V value, int numReduceTasks) {
         		return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
         	}
         }
         ```

         默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。

      2. 自定义Partitioner步骤

         1. 自定义类继承Partitioner，重写getPartition()方法

            ```java
            public class CustomPartitioner extends Partitioner<Text, FlowBean> {
            	@Override
            	public int getPartition(Text key, FlowBean value, int numPartitions) {
            	// 控制分区代码逻辑
            	… …
            		return partition;
            	}
            }
            ```

         2. 在Job驱动中，设置自定义Partitioner

         3. 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask

      3. 分区总结

         1. 如果ReduceTask的数量> getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；
         2. 如果1<ReduceTask的数量<getPartition的结果数，则有一部分分区数据无处安放，会Exception；
         3. 如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000；
         4. 分区号必须从零开始，逐一累加。

      4. 案例分析

         1. 例如：假设自定义分区数为5，则
            - job.setNumReduceTasks(1);会正常运行，只不过会产生一个输出文件
            - job.setNumReduceTasks(2);会报错
            - job.setNumReduceTasks(6);大于5，程序会正常运行，会产生空文件

      5. Partition 分区案例实操

         1. 在序列化案例实操基础上增加一个分区类 `ProvincePartitioner`

            ```java
            package com.lucp.mapreduce.partitioner2;
            
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Partitioner;
            
            // 参数 Text 为手机号
            public class ProvincePartitioner extends Partitioner<Text,FlowBean> {
            
                @Override
                public int getPartition(Text text, FlowBean flowBean, int i) {
                    String phone = text.toString();
                    String perphone = phone.substring(0,3);
            
                    int partition;
                    switch (perphone) {
                        case "136":
                            partition = 0;
                            break;
                        case "137":
                            partition = 1;
                            break;
                        case "138":
                            partition = 2;
                            break;
                        case "139":
                            partition = 3;
                            break;
                        default:
                            partition = 4;
                            break;
                    }
                    //最后返回分区号 partition
                    return partition;
                }
            }
            ```

         2. 在驱动函数中增加自定义数据分区设置和 ReduceTask 设置

            ```java
            package com.lucp.mapreduce.partitioner2;
            
            import org.apache.hadoop.conf.Configuration;
            import org.apache.hadoop.fs.Path;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Job;
            import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
            import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
            
            import java.io.IOException;
            
            public class driveWritable {
                public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
            
                    // 1. 获取 job 对象
                    Configuration conf = new Configuration();
                    Job job = Job.getInstance(conf);
            
                    // 2. 关联本 Driver 类
                    job.setJarByClass(driveWritable.class);
            
                    //3. 关联 Mapper 和 Reducer
                    job.setMapperClass(mapWritable.class);
                    job.setReducerClass(reduceWritable.class);
            
                    // 4. 设置 Map 端输出 KV 类型
                    job.setMapOutputKeyClass(Text.class);
                    job.setMapOutputValueClass(FlowBean.class);
            
                    // 5. 设置程序最终输出的 KV 类型
                    job.setOutputKeyClass(Text.class);
                    job.setOutputValueClass(FlowBean.class);
            
                    ----------------------------------------------------------------------------
                    // 8. 指定自定义数据分区
                    job.setPartitionerClass(ProvincePartitioner.class);
                    // 9. 同时指定相应数量的reduceTask
                    job.setNumReduceTasks(5);
                    ----------------------------------------------------------------------------
            
                    // 6. 设置程序的输入输出路径
                    FileInputFormat.setInputPaths(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\input\\inputPartitioner"));
                    FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\output\\outPartitioner"));
            
                    // 7. 提交 Job
                    boolean b = job.waitForCompletion(true);
                    System.exit(b ? 0 : 1);
                }
            }
            ```

         3. 输出文件

            ![image-20230411144951220](https://s2.loli.net/2023/04/11/82ix5eWPc97UGAR.png)

   3. WritableComparable 排序

      1. 排序概述：排序是MapReduce框架中最重要的操作之一。

         MapTask和ReduceTask均会对数据**按照key**进行排序。该操作属于Hadoop的**默认行为**。**任何应用程序中的数据均会被排序，而不管逻辑上是否需要**。

         默认排序是**按照字典顺序排序**，且实现该排序的方法是**快速排序**。

         对于MapTask，它会将处理的结果暂时放到环形缓冲区中，**当环形缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序**，并将这些有序数据溢写到磁盘上，而**当数据处理完毕后，它会对磁盘上所有文件进行归并排序**。 

         对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件;如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，**ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序**。 

      2. 代码实现

         1. FlowBean 对象在序列化案例实操基础上增加了比较功能

            ```java
            package com.lucp.mapreduce.writablecompable;
            
            import org.apache.hadoop.io.WritableComparable;
            
            import java.io.DataInput;
            import java.io.DataOutput;
            import java.io.IOException;
            
            //1. 继承 Writable 接口
            public class FlowBean implements WritableComparable<FlowBean> {
            
                private long upflow;    // 上行流量
                private long downflow;  // 下行流量
                private long sumflow;   // 总流量
            
                // 2. 空参构造函数
                public FlowBean() {
            
                }
            
                // 3. 提供三个参数的 getter 和 setter 方法
                // 鼠标右键 -> 生成 -> Getter and Setter
                public long getUpflow() {
                    return upflow;
                }
            
                public void setUpflow(long upflow) {
                    this.upflow = upflow;
                }
            
                public long getDownflow() {
                    return downflow;
                }
            
                public void setDownflow(long downflow) {
                    this.downflow = downflow;
                }
            
                public long getSumflow() {
                    return sumflow;
                }
            
                public void setSumflow(long sumflow) {
                    this.sumflow = sumflow;
                }
            
                // 函数重载
                public void setSumflow(){
                    this.sumflow = this.upflow + this.downflow;
                }
            
                // 4. 实现序列化和反序列化方法,注意顺序一定要保持一致
                // 重写序列化方法
                @Override
                public void write(DataOutput dataOutput) throws IOException {
                    dataOutput.writeLong(upflow);
                    dataOutput.writeLong(downflow);
                    dataOutput.writeLong(sumflow);
                }
            
                // 重写反序列化方法
                @Override
                public void readFields(DataInput dataInput) throws IOException {
                    this.upflow = dataInput.readLong();
                    this.downflow = dataInput.readLong();
                    this.sumflow = dataInput.readLong();
                }
            
                // 5. 重写 ToString,最后要输出 FlowBean
                @Override
                public String toString() {
                    return upflow + "\t" + downflow + "\t" + sumflow;
                }
            
                //-----------------------------------------------------------------
                // 6. 按照总流量比较,倒序排列
                @Override
                public int compareTo(FlowBean o){
                    return Long.compare(o.sumflow, this.sumflow);
                    /*
                    if (this.sumflow > o.sumflow){
                        return -1;
                    } else if (this.sumflow < o.sumflow) {
                        return 1;
                    }else {
                        return 0;
                    }
                    */
                }
                //-----------------------------------------------------------------
            }
            ```

         2. 编写 Mapper 类

            ```java
            package com.lucp.mapreduce.writablecompable;
            
            import org.apache.hadoop.io.LongWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Mapper;
            
            import java.io.IOException;
            
            public class mapWritable extends Mapper<LongWritable, Text,FlowBean, Text> {
            
                private Text outV = new Text();
                private FlowBean outK = new FlowBean();
            
                @Override
                protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            
                    // 1. 获取一行数据
                    String line = value.toString();
            
                    // 2. 切割
                    String[] split = line.split("\t");
            
                    // 3. 需要获取手机号以及上下行流量
                    String phone = split[1];
                    String up = split[split.length - 3];
                    String down = split[split.length - 2];
            
                    // 4. 封装 outK outV
                    outV.set(phone);
                    outK.setUpflow(Long.parseLong(up));
                    outK.setDownflow(Long.parseLong(down));
                    outK.setSumflow();
            
                    // 5.写出 outK outV
                    context.write(outK, outV);
            
                }
            }
            ```

         3. 编写 Reducer 类

            ```java
            package com.lucp.mapreduce.writablecompable;
            
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Reducer;
            
            import java.io.IOException;
            
            public class reduceWritable extends Reducer<FlowBean, Text,Text, FlowBean> {
            
                @Override
                protected void reduce(FlowBean key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
                    // 遍历 values 集合,循环写出,避免总流量相同的情况
                    for (Text value : values){
                        // 调换 KV 位置,反向写出
                        context.write(value,key);
                    };
                }
            }
            ```

         4. 编写 Driver 类

            ```java
            package com.lucp.mapreduce.writablecompable;
            
            import org.apache.hadoop.conf.Configuration;
            import org.apache.hadoop.fs.Path;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Job;
            import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
            import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
            
            import java.io.IOException;
            
            public class driveWritable {
                public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
            
                    // 1. 获取 job 对象
                    Configuration conf = new Configuration();
                    Job job = Job.getInstance(conf);
            
                    // 2. 关联本 Driver 类
                    job.setJarByClass(driveWritable.class);
            
                    //3. 关联 Mapper 和 Reducer
                    job.setMapperClass(mapWritable.class);
                    job.setReducerClass(reduceWritable.class);
            
                    //-----------------------------------------------
                    // 4. 设置 Map 端输出 KV 类型
                    job.setMapOutputKeyClass(FlowBean.class);
                    job.setMapOutputValueClass(Text.class);
                    //-----------------------------------------------
                    
                    // 5. 设置程序最终输出的 KV 类型
                    job.setOutputKeyClass(Text.class);
                    job.setOutputValueClass(FlowBean.class);
            
                    // 6. 设置程序的输入输出路径
                    FileInputFormat.setInputPaths(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\input\\inputWritableCompable"));
                    FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\output\\outWritableCompable"));
            
                    // 7. 提交 Job
                    boolean b = job.waitForCompletion(true);
                    System.exit(b ? 0 : 1);
                }
            }
            ```

   4. WritableComparable 排序案例实操（区内排序）

      1. 增加自定义分区类 `inputProvincePartitioner2`

         ```java
         package com.lucp.mapreduce.Partitionercompable;
         
         import org.apache.hadoop.io.Text;
         import org.apache.hadoop.mapreduce.Partitioner;
         
         public class ProvincePartitioner2 extends Partitioner<FlowBean, Text> {
         
             @Override
             public int getPartition(FlowBean flowBean, Text text, int i) {
                 String phone = text.toString();
         
                 String prePhone = phone.substring(0, 3);
         
                 //定义一个分区号变量 partition,根据 prePhone 设置分区号
                 int partition;
                 switch (prePhone) {
                     case "136":
                         partition = 0;
                         break;
                     case "137":
                         partition = 1;
                         break;
                     case "138":
                         partition = 2;
                         break;
                     case "139":
                         partition = 3;
                         break;
                     default:
                         partition = 4;
                         break;
                 }
                 //最后返回分区号 partition
                 return partition;
             }
         }
         ```

      2. 在驱动类中添加分区类

         ```java
         package com.lucp.mapreduce.Partitionercompable;
         
         import org.apache.hadoop.conf.Configuration;
         import org.apache.hadoop.fs.Path;
         import org.apache.hadoop.io.Text;
         import org.apache.hadoop.mapreduce.Job;
         import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
         import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
         
         import java.io.IOException;
         
         public class driveWritable {
             public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
         
                 // 1. 获取 job 对象
                 Configuration conf = new Configuration();
                 Job job = Job.getInstance(conf);
         
                 // 2. 关联本 Driver 类
                 job.setJarByClass(driveWritable.class);
         
                 //3. 关联 Mapper 和 Reducer
                 job.setMapperClass(mapWritable.class);
                 job.setReducerClass(reduceWritable.class);
         
                 // 4. 设置 Map 端输出 KV 类型
                 job.setMapOutputKeyClass(FlowBean.class);
                 job.setMapOutputValueClass(Text.class);
         
                 // 5. 设置程序最终输出的 KV 类型
                 job.setOutputKeyClass(Text.class);
                 job.setOutputValueClass(FlowBean.class);
         
                 //--------------------------------------------------------
                 // 设置自定义分区器
                 job.setPartitionerClass(ProvincePartitioner2.class);
                 // 设置对应的 ReduceTask 的个数
                 job.setNumReduceTasks(5);
                 //--------------------------------------------------------
         
                 // 6. 设置程序的输入输出路径
                 FileInputFormat.setInputPaths(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\input\\inputProvincePartitioner2"));
                 FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\output\\inputProvincePartitioner"));
         
                 // 7. 提交 Job
                 boolean b = job.waitForCompletion(true);
                 System.exit(b ? 0 : 1);
             }
         }
         ```

   5. Combiner 合并

      1. Combiner是MR程序中Mapper和Reducer之外的一种组件。
      2. Combiner组件的父类就是Reducer。
      3. Combiner和Reducer的区别在于运行的位置
         1. Combiner是在每一个MapTask所在的节点运行;
         2. Reducer是接收全局所有Mapper的输出结果；
      4. Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。
      5. Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟Reducer的输入kv类型要对应起来。

   6. 案例实操

      1. 方案一

         1. 增加一个 WordCountCombiner 类继承 Reducer

            ```java
            package com.lucp.mapreduce.WordCountCombiner;
            
            import org.apache.hadoop.io.IntWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Reducer;
            
            import java.io.IOException;
            
            public class WordCountCombiner extends Reducer<Text, IntWritable, Text, IntWritable> {
            
                private IntWritable outV = new IntWritable();
            
                @Override
                protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
                    int sum = 0;
                    for (IntWritable value : values) {
                        sum += value.get();
                    }
                    //封装 outKV
                    outV.set(sum);
                    //写出 outKV
                    context.write(key,outV);
                }
            }
            ```

         2. 在 WordcountDriver 驱动类中指定 Combiner

            ```java
            package com.lucp.mapreduce.WordCountCombiner;
            
            import org.apache.hadoop.conf.Configuration;
            import org.apache.hadoop.fs.Path;
            import org.apache.hadoop.io.IntWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Job;
            import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;
            import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
            import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
            
            import java.io.IOException;
            
            public class WordCountDriver {
            
                public static void main (String[] args) throws IOException, InterruptedException, ClassNotFoundException {
                    // 1. 获取配置信息以及获取 job 对象
                    Configuration conf = new Configuration();
                    Job job = Job.getInstance(conf);
            
                    // 2. 获取Jar包所在路径
                    job.setJarByClass(WordCountDriver.class);
            
                    // 3. 关联mapper和reducer
                    job.setMapperClass(WordCountMapper.class);
                    job.setReducerClass(WordCountReducer.class);
            
                    // 4. 指定map输出的kv类型
                    job.setMapOutputKeyClass(Text.class);
                    job.setMapOutputValueClass(IntWritable.class);
            
                    // 5. 指定最终输出的kv类型
                    job.setOutputKeyClass(Text.class);
                    job.setOutputValueClass(IntWritable.class);
            
                    // 如果不设置 InputFormat，它默认用的是 TextInputFormat.class
                    job.setInputFormatClass(CombineTextInputFormat.class);
                    //虚拟存储切片最大值设置 20m
                    CombineTextInputFormat.setMaxInputSplitSize(job, 20971520);
            
                    //-----------------------------------------------------------------
                    // 指定需要使用 combiner，以及用哪个类作为 combiner 的逻辑
                    job.setCombinerClass(WordCountCombiner.class);
                    //-----------------------------------------------------------------
            
                    // 6. 指定输入输出路径
                    FileInputFormat.setInputPaths(job,new Path("C:\\Users\\Lucp\\Desktop\\dsj\\input\\inputWordCountCombiner"));
                    FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\output\\outputWordCountCombiner"));
            
                    // 7. 提交作业
                    boolean result = job.waitForCompletion(true);
                }
            }
            ```

            结果：

            ```
            Combine input records=42
            Combine output records=42
            ```

      2. 方案二

         1. 将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定

            ```java
            // 指定需要使用 combiner，以及用哪个类作为 combiner 的逻辑
            job.setCombinerClass(WordCountCombiner.class);
            ```

         2. 运行程序，如下图所示![image-20230412113425458](https://s2.loli.net/2023/04/12/8Med2SHKVxEWUAX.png)

4. OutputFormat 数据输出

   1. OutputFormat 接口实现类

   2. 自定义 OutputFormat 案例实操

      1. 需求：过滤输入的 log 日志，包含 itwork的网站输出到 e:/itwork.log，不包含 itwork 的网站输出到 e:/other.log。![image-20230413091008969](https://s2.loli.net/2023/04/13/B8eRJ7Yj16v4pUb.png)

      2. 案例实操

         1. 编写 LogMapper 类 `LogMapper.java`

            ```java
            package com.lucp.mapreduce.Outputformat;
            
            import org.apache.hadoop.io.LongWritable;
            import org.apache.hadoop.io.NullWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Mapper;
            
            import java.io.IOException;
            
            public class LogMapper extends Mapper<LongWritable, Text,Text, NullWritable> {
                @Override
                protected void map(LongWritable key,Text value,Context context) throws IOException, InterruptedException {
                    // 不做处理，仅作读入
                    context.write(value,NullWritable.get());
                }
            }
            ```

         2. 编写 LogReducer 类 `LogReducer.java`

            ```java
            package com.lucp.mapreduce.Outputformat;
            
            import org.apache.hadoop.io.NullWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Reducer;
            
            import java.io.IOException;
            
            public class LogReducer extends Reducer<Text, NullWritable,Text,NullWritable> {
                @Override
                protected void reduce(Text key, Iterable<NullWritable> values, Reducer<Text, NullWritable, Text, NullWritable>.Context context) throws IOException, InterruptedException {
            
                    // 防止有相同的数据,遍历写出
                    for (NullWritable value : values) {
                        context.write(key,NullWritable.get());
                    }
                }
            }
            ```

         3. 自定义一个 LogOutputFormat 类 `LogOutputFormat.java`

            ```java
            package com.lucp.mapreduce.Outputformat;
            
            import org.apache.hadoop.io.NullWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.RecordWriter;
            import org.apache.hadoop.mapreduce.TaskAttemptContext;
            import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
            
            import java.io.IOException;
            
            public class LogOutputFormat extends FileOutputFormat<Text, NullWritable> {
                @Override
                public RecordWriter<Text, NullWritable> getRecordWriter(TaskAttemptContext job) throws IOException, InterruptedException {
                    return new LogRecordWriter(job);
                }
            }
            ```

         4. 编写 LogRecordWriter 类 `LogRecordWriter.java`

            ```java
            package com.lucp.mapreduce.Outputformat;
            
            import org.apache.hadoop.fs.FSDataOutputStream;
            import org.apache.hadoop.fs.FileSystem;
            import org.apache.hadoop.fs.Path;
            import org.apache.hadoop.io.IOUtils;
            import org.apache.hadoop.io.NullWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.RecordWriter;
            import org.apache.hadoop.mapreduce.TaskAttemptContext;
            
            import java.io.IOException;
            
            public class LogRecordWriter extends RecordWriter<Text, NullWritable> {
            
            
                private FSDataOutputStream itworkOut;
                private FSDataOutputStream otherOut;
            
                public LogRecordWriter(TaskAttemptContext job){
                    try {
                        // 获取文件系统对象
                        FileSystem fs = FileSystem.get(job.getConfiguration());
                        // 用文件系统对象创建两个输出流对应不同的目录
                        itworkOut = fs.create(new Path("C:\\Users\\Lucp\\Desktop\\dsj\\output\\logoutput\\itwork.log"));
                        otherOut = fs.create(new Path("C:\\Users\\Lucp\\Desktop\\dsj\\output\\logoutput\\other.log"));
                    } catch (IOException e) {
                        throw new RuntimeException(e);
                    }
            
                }
            
                @Override
                public void write(Text key, NullWritable value) throws IOException, InterruptedException {
                    String logData = key.toString();
                    // 根据一行的 log 数据是否包含 itWork,判断属于哪条流
                    if (logData.contains("itwork")) {
                        itworkOut.writeBytes(logData+ "\n");
                    }else{
                        otherOut.writeBytes(logData+ "\n");
                    }
                }
            
                @Override
                public void close(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
            
                    // 关流
                    IOUtils.closeStream(itworkOut);
                    IOUtils.closeStream(otherOut);
            
                }
            }
            ```

         5. 编写 LogDriver 类

            ```java
            package com.lucp.mapreduce.Outputformat;
            
            import org.apache.hadoop.conf.Configuration;
            import org.apache.hadoop.fs.Path;
            import org.apache.hadoop.io.NullWritable;
            import org.apache.hadoop.io.Text;
            import org.apache.hadoop.mapreduce.Job;
            import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
            import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
            
            import java.io.IOException;
            
            public class LogDriver {
                public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
                    Configuration conf = new Configuration();
                    Job job = Job.getInstance(conf);
            
                    job.setJarByClass(LogDriver.class);
                    job.setMapperClass(LogMapper.class);
                    job.setReducerClass(LogReducer.class);
            
                    job.setMapOutputKeyClass(Text.class);
                    job.setMapOutputValueClass(NullWritable.class);
            
                    job.setOutputKeyClass(Text.class);
                    job.setOutputValueClass(NullWritable.class);
            
                    //设置自定义的 outputFormat
                    job.setOutputFormatClass(LogOutputFormat.class);
                    FileInputFormat.setInputPaths(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\input\\InputOutputFormat"));
                    // 虽然我们自定义了 outputFormat ，但是因为我们的outputFormat 继承自fileOutputFormat
                    //而 fileOutputFormat 要输出一个_SUCCESS 文件，所以在这还得指定一个输出目录
                    FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\output\\logoutput"));
                    boolean b = job.waitForCompletion(true);
                    System.exit(b ? 0 : 1);
                }
            }
            ```

         6.  OutputFormat 案例实操结果![image-20230413102815709](https://s2.loli.net/2023/04/13/l7witHe1PYvkgWL.png)

5. MapReduce 内核源码解析

   1. MapTask工作机制![image-20230413113529337](https://s2.loli.net/2023/04/13/prdUxCkGBMZaLP7.png)

   2. ReduceTask 工作机制![image-20230413113703296](https://s2.loli.net/2023/04/13/wylYQNpSZT2b7LI.png)

   3. ReduceTask 并行度决定机制

      1. 设置 ReduceTask 并行度（个数）

         ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并 发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置：

         ```java
         // 默认值是 1，手动设置为 4
         job.setNumReduceTasks(4);
         ```

      2. 注意事项

         1. ReduceTask=0，表示没有Reduce阶段，输出文件个数和Map个数一致。 
         2. ReduceTask默认值就是1，所以输出文件个数为一个。 
         3. 如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜 。
         4. ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个ReduceTask。
         5. 具体多少个ReduceTask，需要根据集群性能而定。
         6. 如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1肯定不执行。

6. Join 应用

   1. `TableDrive`

      ```java
      package com.lucp.mapreduce.ReduceJoin;
      
      import org.apache.hadoop.io.Writable;
      
      import java.io.DataInput;
      import java.io.DataOutput;
      import java.io.IOException;
      
      public class TableBean implements Writable {
      
          private String id;  // 订单ID
          private String pid;  // 商品ID
          int amount;  // 商品数量
          private String pname;  // 商品名称
          private String flag;  // 判断是 order 表还是 pd 表的标志字段
      
          public TableBean() {
          }
      
          public String getId() {
              return id;
          }
      
          public void setId(String id) {
              this.id = id;
          }
      
          public String getPid() {
              return pid;
          }
      
          public void setPid(String pid) {
              this.pid = pid;
          }
      
          public int getAmount() {
              return amount;
          }
      
          public void setAmount(int amount) {
              this.amount = amount;
          }
      
          public String getPname() {
              return pname;
          }
      
          public void setPname(String pname) {
              this.pname = pname;
          }
      
          public String getFlag() {
              return flag;
          }
      
          public void setFlag(String flag) {
              this.flag = flag;
          }
      
          @Override
          public void write(DataOutput out) throws IOException {
              out.writeUTF(id);
              out.writeUTF(pid);
              out.writeInt(amount);
              out.writeUTF(pname);
              out.writeUTF(flag);
          }
      
          @Override
          public void readFields(DataInput in) throws IOException {
              this.id = in.readUTF();
              this.pid = in.readUTF();
              this.amount = in.readInt();
              this.pname = in.readUTF();
              this.flag = in.readUTF();
          }
      
          @Override
          public String toString(){
              return id + "\t" + pname + "\t" + amount;
          }
      }
      ```

   2. 编写 TableMapper 类

      ```java
      package com.lucp.mapreduce.ReduceJoin;
      
      import org.apache.hadoop.io.LongWritable;
      import org.apache.hadoop.io.Text;
      import org.apache.hadoop.mapreduce.InputSplit;
      import org.apache.hadoop.mapreduce.Mapper;
      import org.apache.hadoop.mapreduce.lib.input.FileSplit;
      
      import java.io.IOException;
      
      public class TableMapper extends Mapper<LongWritable, Text,Text,TableBean> {
          private String filename;
          private Text outK = new Text();
          private TableBean outV = new TableBean();
      
      
          @Override
          protected void setup(Context context) throws IOException, InterruptedException {
              //获取对应文件名称
              InputSplit split = context.getInputSplit();
              FileSplit fileSplit = (FileSplit) split;
              filename = fileSplit.getPath().getName();
          }
      
      
          @Override
          protected void map(LongWritable key, Text value, Context context)
                  throws IOException, InterruptedException {
              //获取一行
              String line = value.toString();
              //判断是哪个文件,然后针对文件进行不同的操作
              if (filename.contains("order")) { //订单表的处理
                  String[] split = line.split("\t");
                  //封装 outK
                  outK.set(split[1]);
                  //封装 outV
                  outV.setId(split[0]);
                  outV.setPid(split[1]);
                  outV.setAmount(Integer.parseInt(split[2]));
                  outV.setPname("");
                  outV.setFlag("order");
              } else { //商品表的处理
                  String[] split = line.split("\t");
                  //封装 outK
                  outK.set(split[0]);
                  //封装 outV
                  outV.setId("");
                  outV.setPid(split[0]);
                  outV.setAmount(0);
                  outV.setPname(split[1]);
                  outV.setFlag("pd");
              }
              //写出 KV
              context.write(outK, outV);
          }
      }
      ```

   3. 编写 TableReducer 类

      ```java
      package com.lucp.mapreduce.ReduceJoin;
      
      import org.apache.commons.beanutils.BeanUtils;
      import org.apache.hadoop.io.NullWritable;
      import org.apache.hadoop.io.Text;
      import org.apache.hadoop.mapreduce.Reducer;
      
      import java.io.IOException;
      import java.lang.reflect.InvocationTargetException;
      import java.util.ArrayList;
      
      public class TableReducer extends Reducer<Text,TableBean,TableBean, NullWritable> {
      
      
          @Override
          protected void reduce(Text key, Iterable<TableBean> values, Context context) throws IOException, InterruptedException {
      
              ArrayList<TableBean> orderBeans = new ArrayList<>();
              TableBean pdBean = new TableBean();
      
              for (TableBean value : values) {
                  //判断数据来自哪个表
                  if("order".equals(value.getFlag())){ //订单表
                      //创建一个临时 TableBean 对象接收 value
                      TableBean tmpOrderBean = new TableBean();
                      try {
                          BeanUtils.copyProperties(tmpOrderBean,value);
                      } catch (IllegalAccessException | InvocationTargetException e) {
                          e.printStackTrace();
                      }
                      //将临时 TableBean 对象添加到集合 orderBeans
                      orderBeans.add(tmpOrderBean);
                  }else { //商品表
                      try {
                          BeanUtils.copyProperties(pdBean,value);
                      } catch (IllegalAccessException | InvocationTargetException e) {
                          e.printStackTrace();
                      }
                  }
              }
      
          }
      }
      ```

   4. 编写 TableDriver 类

      ```java
      package com.lucp.mapreduce.ReduceJoin;
      
      import org.apache.hadoop.conf.Configuration;
      import org.apache.hadoop.fs.Path;
      import org.apache.hadoop.io.NullWritable;
      import org.apache.hadoop.io.Text;
      import org.apache.hadoop.mapreduce.Job;
      import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
      import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
      
      import java.io.IOException;
      
      public class TableDriver {
          public static void main(String[] args) throws IOException,
                  ClassNotFoundException, InterruptedException {
              Job job = Job.getInstance(new Configuration());
              job.setJarByClass(TableDriver.class);
              job.setMapperClass(TableMapper.class);
              job.setReducerClass(TableReducer.class);
              job.setMapOutputKeyClass(Text.class);
              job.setMapOutputValueClass(TableBean.class);
              job.setOutputKeyClass(TableBean.class);
              job.setOutputValueClass(NullWritable.class);
              FileInputFormat.setInputPaths(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\input\\inputReduceJoin"));
              FileOutputFormat.setOutputPath(job, new Path("C:\\Users\\Lucp\\Desktop\\dsj\\output\\outputReduceJoin"));
              boolean b = job.waitForCompletion(true);
              System.exit(b ? 0 : 1);
          }
      }
      ```

7. Map Join

   1. Map Join 适用于一张表十分小、一张表很大的场景

   2. 具体办法：采用 DistributedCache

      1. 在 Mapper 的 setup 阶段，将文件读取到缓存集合中。

      2. 在 Driver 驱动类中加载缓存.

         ```java
         //缓存普通文件到 Task 运行节点。
         job.addCacheFile(new URI("file:///e:/cache/pd.txt"));
         //如果是集群运行,需要设置 HDFS 路径
         job.addCacheFile(new URI("hdfs://hadoop102:8020/cache/pd.txt"));
         ```


8. 数据清洗（ETL）

   1. “ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取 （Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓 库，但其对象并不限于数据仓库.在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户 要求的数据。**清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序**。

   2. 实现代码

      1. 编写 WebLogMapper 类

         ```java
         package com.lucp.mapreduce.ETL;
         
         import org.apache.hadoop.io.LongWritable;
         import org.apache.hadoop.io.NullWritable;
         import org.apache.hadoop.io.Text;
         import org.apache.hadoop.mapreduce.Mapper;
         
         import java.io.IOException;
         
         public class LogMapper extends Mapper<LongWritable, Text,Text, NullWritable> {
             @Override
             protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {
                 // 1 获取一行
                 String line = value.toString();
         
                 // 2 ETL
                 boolean result = parseLog(line,context);
         
                 // 3 日志不合法退出
                 if(!result){
                     return;
                 }
                 // 4 日志合法就直接写出
                 context.write(value, NullWritable.get());
             }
         
             private boolean parseLog(String line, Mapper<LongWritable, Text, Text, NullWritable>.Context context) {
         
                 // 1 截取
                 String[] fields = line.split(" ");
         
                 // 2 日志长度大于 11 的为合法
                 if (fields.length > 11) {
                     return true;
                 }else {
                     return false;
                 }
             }
         }
         ```

      2. 编写 WebLogDriver 类

         ```java
         package com.lucp.mapreduce.ETL;
         
         import org.apache.hadoop.conf.Configuration;
         import org.apache.hadoop.fs.Path;
         import org.apache.hadoop.io.NullWritable;
         import org.apache.hadoop.io.Text;
         import org.apache.hadoop.mapreduce.Job;
         import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
         import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
         
         public class LogDriver {
             public static void main(String[] args) throws Exception {
                 // 输入输出路径需要根据自己电脑上实际的输入输出路径设置
                 args = new String[] { "C:\\Users\\Lucp\\Desktop\\dsj\\input\\inputETL", "C:\\Users\\Lucp\\Desktop\\dsj\\output\\outputETL" };
                 // 1 获取 job 信息
                 Configuration conf = new Configuration();
                 Job job = Job.getInstance(conf);
                 // 2 加载 jar 包
                 job.setJarByClass(LogDriver.class);
                 // 3 关联 map
                 job.setMapperClass(LogMapper.class);
                 // 4 设置最终输出类型
                 job.setOutputKeyClass(Text.class);
                 job.setOutputValueClass(NullWritable.class);
                 // 设置 reduceTask 个数为 0
                 job.setNumReduceTasks(0);
                 // 5 设置输入和输出路径
                 FileInputFormat.setInputPaths(job, new Path(args[0]));
                 FileOutputFormat.setOutputPath(job, new Path(args[1]));
                 // 6 提交
                 boolean b = job.waitForCompletion(true);
                 System.exit(b ? 0 : 1);
             }
         
         }
         ```

### 3.4 压缩

1. 概述
2. MR 支持的压缩编码
3. 压缩方式选择
   1. Gzip 压缩
   2. Bzip2 压缩
   3. Lzo 压缩
   4. Snappy 压缩
   5. 压缩位置选择![image-20230414112711657](C:\Users\Lucp\AppData\Roaming\Typora\typora-user-images\image-20230414112711657.png)
4. 压缩参数配置
5. 压缩实操案例
   1. Map 输出端采用压缩
   2. Reduce 输出端采用压缩

## 4. Yarn

### 4.1 Yarn 资源调度器

#### 4.1.1 Yarn 基础架构

YARN 主要由 ResourceManager、NodeManager、ApplicationMaster 和 Container 等组件 构成。![image-20230417093133856](https://s2.loli.net/2023/04/17/HvrJz1ILCUunKeM.png)

#### 4.1.2 Yarn 工作机制

![image-20230417093253635](https://s2.loli.net/2023/04/17/fYx6W8UAp9khJ5d.png)

#### 4.1.3 作业提交全过程

HDFS、YARN、MapReduce三者关系![image-20230417093413594](https://s2.loli.net/2023/04/17/EnOd685kNuQa34Y.png)

作业提交过程之YARN ![image-20230417093507768](https://s2.loli.net/2023/04/17/GjobScXxBCuRZQm.png)

作业提交过程之HDFS & MapReduce ![image-20230417093539861](https://s2.loli.net/2023/04/17/VXZmfcWovFBlHey.png)

#### 4.1.4 Yarn 调度器和调度算法

目前，Hadoop 作业调度器主要有三种：FIFO、容量（Capacity Scheduler）和公平（Fair  Scheduler）。Apache Hadoop3.1.3 默认的资源调度器是 **Capacity Scheduler**。

CDH 框架默认调度器是 Fair Scheduler。

1. 先进先出调度器（FIFO）：FIFO 调度器（First In First Out）：单队列，根据提交作业的先后顺序，先来先服务。

   ![image-20230417093724253](https://s2.loli.net/2023/04/17/XQwqx2vV4LCGHTc.png)

2. 容量调度器（Capacity Scheduler）

   Capacity Scheduler 是 Yahoo 开发的多用户调度器。![image-20230417094116154](https://s2.loli.net/2023/04/17/YST8hZoy1DaNVnC.png)

3. 公平调度器（Fair Scheduler）

   ![image-20230417100925055](https://s2.loli.net/2023/04/17/21dKWg5aJH4msir.png)

   公平调度器设计目标是：在时间尺度上，所有作业获得公平的资源。某一时刻一个作业应获资源和实际获取资源的差距叫“缺额”。
   调度器会优先为缺额大的作业分配资源 。

   公平调度器队列资源分配方式：![image-20230417110328989](https://s2.loli.net/2023/04/17/WziVCtA1JoxOb8q.png)

#### 4.1.5 Yarn 常用命令

Yarn 状态的查询，除了可以在 http://hadoop103:8088/ 页面查看外，还可以通过命令操作。常见的命令操作如下所示：

执行 WordCount 案例，并用 Yarn 命令查看任务运行情况。

1. 列出所有 Application：

   ```bash
   [lucp@hadoop102 ~]$ cd /opt/module/hadoop-3.1.3/
   [lucp@hadoop102 hadoop-3.1.3]$ myhadoop.sh start
   [lucp@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /wcinput /wcoutput
   ```

![image-20230417113215070](https://s2.loli.net/2023/04/17/lVnZ9RUAJfrGQDv.png)

2. 根据 Application 状态过滤：yarn application -list -appStates （所有状态：ALL、NEW、 NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）

```bash
[lucp@hadoop102 hadoop-3.1.3]$ yarn application -list -appStates FINISHED
2023-04-17 11:38:55,510 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032
Total number of applications (application-types: [], states: [FINISHED] and tags: []):1
Application-Id      					Application-Name        Application-Type      User           Queue   
application_1681702209610_0002            word count               MAPREDUCE          lucp         default 
State             Final-State              Progress                        Tracking-URL
FINISHED          SUCCEEDED                  100% 					http://hadoop102:19888/jobhistory/job/job_1681702209610_0002
```

3. Kill 掉 Application：

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$ yarn application -kill application_1681702209610_0002
   ```

4. yarn logs 查看日志

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$ yarn logs -applicationId application_1681709206052_0002
   ```

5. yarn applicationattempt 查看尝试运行的任务

   1. 列出所有 Application 尝试的列表：yarn applicationattempt -list 

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ yarn applicationattempt -list application_1681709206052_0002
      2023-04-17 13:32:54,042 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032
      Total number of application attempts :1
               ApplicationAttempt-Id                 State                        AM-Container-Id                            Tracking-URL
      appattempt_1681709206052_0002_000001                FINISHED    container_1681709206052_0002_01_000001  http://hadoop103:8088/proxy/application_1681709206052_0002/
      ```

   2. 打印 ApplicationAttemp 状态：yarn applicationattempt -status 

      ```bash
      [lucp@hadoop102 hadoop-3.1.3]$ yarn applicationattempt -status application_1681709206052_0002
      2023-04-17 13:34:25,284 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032
      Exception in thread "main" java.lang.IllegalArgumentException: Invalid AppAttemptId prefix: application_1681709206052_0002
              at org.apache.hadoop.yarn.api.records.ApplicationAttemptId.fromString(ApplicationAttemptId.java:144)
              at org.apache.hadoop.yarn.client.cli.ApplicationCLI.printApplicationAttemptReport(ApplicationCLI.java:844)
              at org.apache.hadoop.yarn.client.cli.ApplicationCLI.run(ApplicationCLI.java:412)
              at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)
              at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)
              at org.apache.hadoop.yarn.client.cli.ApplicationCLI.main(ApplicationCLI.java:123)
      ```

6. yarn container 查看容器

   1. 列出所有 Container：yarn container -list 
   2. 打印 Container 状态：yarn container -status 

   > 注：只有在任务跑的途中才能看到 container 的状态

7. yarn node 查看节点状态

   列出所有节点：yarn node -list -all

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$ yarn node -list -all
   2023-04-17 13:36:14,284 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032
   Total Nodes:3
            Node-Id             Node-State Node-Http-Address       Number-of-Running-Containers
    hadoop103:39464                RUNNING    hadoop103:8042                                  0
    hadoop102:44237                RUNNING    hadoop102:8042                                  0
    hadoop104:44099                RUNNING    hadoop104:8042                                  0
   ```

8. yarn rmadmin 更新配置

   加载队列配置：yarn rmadmin -refreshQueues

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$ yarn rmadmin -refreshQueues
   2023-04-17 13:37:09,957 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8033
   ```

9. yarn queue 查看队列

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$  yarn queue -status default
   2023-04-17 13:41:13,648 INFO client.RMProxy: Connecting to ResourceManager at hadoop103/192.168.10.103:8032
   Queue Information : 
   Queue Name : default
           State : RUNNING
           Capacity : 100.0%
           Current Capacity : .0%
           Maximum Capacity : 100.0%
           Default Node Label expression : <DEFAULT_PARTITION>
           Accessible Node Labels : *
           Preemption : disabled
           Intra-queue Preemption : disabled
   ```

#### 4.1.6 Yarn 生产环境核心参数

![image-20230417145736620](https://s2.loli.net/2023/04/17/64C7RNhimbXMPI2.png)

### 4.2 Yarn 案例实操

> 注：调整下列参数之前尽量拍摄 Linux 快照，否则后续的案例，还需要重写准备集群。

#### 4.2.1 Yarn 生产环境核心参数配置案例

1. 需求：从 1G 数据中，统计每个单词出现次数。服务器 3 台，每台配置 4G 内存，4 核 CPU，4 线程。

2. 需求分析： 1G / 128m = 8 个 MapTask；1 个 ReduceTask；1 个 mrAppMaster 平均每个节点运行 10 个 / 3 台 ≈ 3 个任务（4 3 3）

3. 修改 yarn-site.xml 

   ```bash
   [lucp@hadoop102 hadoop-3.1.3]$ cd $HADOOP_HOME/etc/hadoop
   [lucp@hadoop102 hadoop]$ vim yarn-site.xml 
   ```

   配置参数如下：

   ```xml
   <!-- 选择调度器，默认容量 -->
   <property>
       <description>The class to use as the resource scheduler.</description>
       <name>yarn.resourcemanager.scheduler.class</name>
       <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
   </property>
   
   <!-- ResourceManager 处理调度器请求的线程数量,默认 50；如果提交的任务数大于 50，可以增加该值，但是不能超过 3 台 * 4 线程 = 12 线程（去除其他应用程序实际不能超过 8） -->
   <property>
       <description>Number of threads to handle scheduler interface.</description>
       <name>yarn.resourcemanager.scheduler.client.thread-count</name>
       <value>8</value>
   </property>
   
   <!-- 是否让 yarn 自动检测硬件进行配置，默认是 false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 -->
   <property>
       <description>Enable auto-detection of node capabilities such as memory and CPU.</description>
       <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
       <value>false</value>
   </property>
   
   <!-- 是否将虚拟核数当作 CPU 核数，默认是 false，采用物理 CPU 核数 -->
   <property>
       <description>Flag to determine if logical processors(such as hyperthreads) should be counted as cores. Only applicable on Linux when yarn.nodemanager.resource.cpu-vcores is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true.
       </description>
       <name>yarn.nodemanager.resource.count-logical-processors-ascores</name>
       <value>false</value>
   </property>
   <!-- 虚拟核数和物理核数乘数，默认是 1.0 -->
   <property>
       <description>Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The number of vcores will be calculated as number of CPUs * multiplier.
       </description>
       <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
       <value>1.0</value>
   </property>
   <!-- NodeManager 使用内存数，默认 8G，修改为 4G 内存 -->
   <property>
       <description>Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB.
       </description>
       <name>yarn.nodemanager.resource.memory-mb</name>
       <value>4096</value>
   </property>
   
   <!-- nodemanager 的 CPU 核数，不按照硬件环境自动设定时默认是 8 个，修改为 2 个，根据服务器实际情况来定 -->
   <property>
       <description>Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of CPUs used by YARN containers. If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically determined from the hardware in case of Windows and Linux. In other cases, number of vcores is 8 by default.
       </description>
       <name>yarn.nodemanager.resource.cpu-vcores</name>
       <value>2</value>
   </property>
   
   <!-- 容器最小内存，默认 1G -->
   <property>
       <description>The minimum allocation for every container request at the RM in MBs. Memory requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have less memory than this value will be shut down by the resource manager.
       </description>
       <name>yarn.scheduler.minimum-allocation-mb</name>
       <value>1024</value>
   </property>
   
   <!-- 容器最大内存，默认 8G，修改为 2G；总容量除以容器数量 -->
   <property>
       <description>The maximum allocation for every container request at the RM in MBs. Memory requests higher than this will throw an InvalidResourceRequestException.
       </description>
       <name>yarn.scheduler.maximum-allocation-mb</name>
       <value>2048</value>
   </property>
   
   <!-- 容器最小 CPU 核数，默认 1 个 -->
   <property>
       <description>The minimum allocation for every container request at the RM in terms of virtual CPU cores. Requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have fewer virtual cores than this value will be shut down by the resource manager.
       </description>
       <name>yarn.scheduler.minimum-allocation-vcores</name>
       <value>1</value>
   </property>
   
   <!-- 容器最大 CPU 核数，默认 4 个，修改为 2 个 -->
   <property>
       <description>The maximum allocation for every container request at the RM in terms of virtual CPU cores. Requests higher than this will throw an InvalidResourceRequestException.
       </description>
       <name>yarn.scheduler.maximum-allocation-vcores</name>
       <value>2</value>
   </property>
   
   <!-- 虚拟内存检查，默认打开，修改为关闭 -->
   <property>
       <description>Whether virtual memory limits will be enforced for containers.
       </description>
       <name>yarn.nodemanager.vmem-check-enabled</name>
       <value>false</value>
   </property>
   
   <!-- 虚拟内存和物理内存设置比例,默认 2.1 -->
   <property>
       <description>Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.
       </description>
       <name>yarn.nodemanager.vmem-pmem-ratio</name>
       <value>2.1</value>
   </property>
   ```

4. 分发配置

   ```bash
   [lucp@hadoop102 hadoop]$ pwd
   /opt/module/hadoop-3.1.3/etc/hadoop
   [lucp@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop
   ```

5. 重启集群

   ```bash
   [lucp@hadoop102 hadoop]$ myhadoop.sh stop
   [lucp@hadoop102 hadoop]$ myhadoop.sh start
   ```

6. 测试

   ```bash
   [lucp@hadoop102 hadoop]$ cd /opt/module/hadoop-3.1.3/
   [lucp@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /wcnewinput /wcnewoutput
   ```

7. 观察 Yarn 任务执行页面[All Applications](http://hadoop103:8088/cluster/apps)

   ![image-20230417154353599](https://s2.loli.net/2023/04/17/uZMOebh7CvHWYIS.png)

#### 4.2.2 容量调度器多队列提交案例

1. 需求 

   1. default 队列占总内存的 40%，最大资源容量占总资源 60%，hive 队列占总内存 的 60%，最大资源容量占总资源 80%。 
   2. 配置队列优先级

2. 配置多队列的容量调度器

   1. 在 capacity-scheduler.xml  `/opt/module/hadoop-3.1.3/etc/hadoop/` 中配置如下：

      ```xml
      <!-- 修改如下配置 -->
      <!-- 指定多队列，增加 hive 队列 -->
      <property>
       <name>yarn.scheduler.capacity.root.queues</name>
       <value>default,hive</value>
       <description>
       The queues at the this level (root is the root queue).
       </description>
      </property>
      <!-- 降低 default 队列资源额定容量为 40%，默认 100% -->
      <property>
       <name>yarn.scheduler.capacity.root.default.capacity</name>
       <value>40</value>
      </property>
      <!-- 降低 default 队列资源最大容量为 60%，默认 100% -->
      <property>
       <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
       <value>60</value>
      </property>
      
      
      
      <!-- 为新加队列添加必要属性 -->
      <!-- 指定 hive 队列的资源额定容量 -->
      <property>
       <name>yarn.scheduler.capacity.root.hive.capacity</name>
       <value>60</value>
      </property>
      <!-- 用户最多可以使用队列多少资源，1 表示 -->
      <property>
       <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
       <value>1</value>
      </property>
      <!-- 指定 hive 队列的资源最大容量 -->
      <property>
       <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
       <value>80</value>
      </property>
      <!-- 启动 hive 队列 -->
      <property>
       <name>yarn.scheduler.capacity.root.hive.state</name>
       <value>RUNNING</value>
      </property>
      <!-- 哪些用户有权向队列提交作业 -->
       
      <property>
       <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
       <value>*</value>
      </property>
      <!-- 哪些用户有权操作队列，管理员权限（查看/杀死） -->
       
      <property>
       <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
       <value>*</value>
      </property>
      <!-- 哪些用户有权配置提交任务优先级 -->
      <property>
       
      <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
       <value>*</value>
      </property>
      <!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout
      参考资料： https://blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ -->
      <!-- 如果 application 指定了超时时间，则提交到该队列的 application 能够指定的最大超时
      时间不能超过该值。
      -->
      <property>
       <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime</name>
       <value>-1</value>
      </property>
      <!-- 如果 application 没指定超时时间，则用 default-application-lifetime 作为默认
      值 -->
      <property>
       <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>
       <value>-1</value>
      </property>
      ```

   2. 分发配置文件

      ```bash
      [itwork@hadoop102 hadoop]$ pwd
      /opt/module/hadoop-3.1.3/etc/hadoop
      [itwork@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop
      ```

   3. 重启 Yarn 或者执行 yarn rmadmin -refreshQueues 刷新队列，就可以看到两条队列：

      ```bash
      [itwork@hadoop102 hadoop-3.1.3]$ yarn rmadmin -refreshQueues
      ```

#### 4.2.3 向 Hive 队列提交任务

1. hadoop jar 的方式

   ```bash
   [itwork@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D 
   mapreduce.job.queuename=hive /input /output
   ```

   > 注: -D 表示运行时改变参数值

2. 打 jar 包的方式

   默认的任务提交都是提交到 default 队列的。如果希望向其他队列提交任务，需要在 Driver 中声明：

   ```java
   public class WcDrvier {
    public static void main(String[] args) throws IOException, 
   ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    conf.set("mapreduce.job.queuename","hive");
    //1. 获取一个 Job 实例
    Job job = Job.getInstance(conf);
    ......
    //6. 提交 Job
    boolean b = job.waitForCompletion(true);
    System.exit(b ? 0 : 1);
    }
   }
   ```

   这样，这个任务在集群提交时，就会提交到 hive 队列：![image-20230418093707439](https://s2.loli.net/2023/04/18/ba4gQXhqWfTkxRd.png)

#### 4.2.4 任务优先级

容量调度器，支持任务优先级的配置，在资源紧张时，优先级高的任务将优先获取资源。 默认情况，Yarn 将所有任务的优先级限制为 0，若想使用任务的优先级功能，须开放该限制。

1. 修改 yarn-site.xml 文件，增加以下参数

   ```xml
   <property>
    <name>yarn.cluster.max-application-priority</name>
    <value>5</value>
   </property>
   ```

2. 分发配置，并重启 Yarn

   ```bash
   [itwork@hadoop102 hadoop]$ xsync yarn-site.xml
   [itwork@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh
   [itwork@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh
   ```

3. 模拟资源紧张环境，**可连续提交以下任务，直到新提交的任务申请不到资源为止**。

   ```
   [itwork@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 5 2000000
   ```

4. 再次重新提交优先级高的任务

   ```bash
   [itwork@hadoop102 hadoop-3.1.3]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -D mapreduce.job.priority=5 5 2000000
   ```

5. 也可以通过以下命令修改正在执行的任务的优先级。

   ```bash
   [itwork@hadoop102 hadoop-3.1.3]$ yarn application -appID application_1611133087930_0009 -updatePriority 5
   ```

### 4.3 公平调度器案例

1. 修改 yarn-site.xml 文件，加入以下参数

   ```xml
   <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    
   <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairS
   cheduler</value>
    <description>配置使用公平调度器</description>
   </property>
   <property>
    
    
   <name>yarn.scheduler.fair.allocation.file</name>
   <value>/opt/module/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml</value>
    <description>指明公平调度器队列分配配置文件</description>
   </property>
   <property>
    <name>yarn.scheduler.fair.preemption</name>
    <value>false</value>
    <description>禁止队列间资源抢占</description>
   </property>
   ```

2. 配置 fair-scheduler.xml

   ```xml
   <?xml version="1.0"?>
    
   <allocations>
   <!-- 单个队列中 Application Master 占用资源的最大比例,取值 0-1 ，企业一般配置 0.1 
   -->
    <queueMaxAMShareDefault>0.5</queueMaxAMShareDefault>
    
    
   <!-- 单个队列最大资源的默认值 test itwork default -->
   <queueMaxResourcesDefault>4096mb,4vcores</queueMaxResourcesDefault>
    <!-- 增加一个队列 test -->
    <queue name="test">
    <!-- 队列最小资源 -->
    <minResources>2048mb,2vcores</minResources>
    <!-- 队列最大资源 -->
    <maxResources>4096mb,4vcores</maxResources>
    <!-- 队列中最多同时运行的应用数，默认 50，根据线程数配置 -->
    <maxRunningApps>4</maxRunningApps>
    <!-- 队列中 Application Master 占用资源的最大比例 -->
    <maxAMShare>0.5</maxAMShare>
    <!-- 该队列资源权重,默认值为 1.0 -->
    <weight>1.0</weight>
    <!-- 队列内部的资源分配策略 -->
    <schedulingPolicy>fair</schedulingPolicy>
    </queue>
    <!-- 增加一个队列 itwork -->
    <queue name="itwork" type="parent">
    <!-- 队列最小资源 -->
    <minResources>2048mb,2vcores</minResources>
    <!-- 队列最大资源 -->
    <maxResources>4096mb,4vcores</maxResources>
    <!-- 队列中最多同时运行的应用数，默认 50，根据线程数配置 -->
    <maxRunningApps>4</maxRunningApps>
    <!-- 队列中 Application Master 占用资源的最大比例 -->
    <maxAMShare>0.5</maxAMShare>
    <!-- 该队列资源权重,默认值为 1.0 -->
    <weight>1.0</weight>
    <!-- 队列内部的资源分配策略 -->
    <schedulingPolicy>fair</schedulingPolicy>
    </queue>
    <!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 -->
    
    
   <queuePlacementPolicy>
   <!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false 表示：如果指
   定队列不存在,不允许自动创建-->
    
    
   <rule name="specified" create="false"/>
   <!-- 提交到 root.group.username 队列,若 root.group 不存在,不允许自动创建；若
   root.group.user 不存在,允许自动创建 -->
    <rule name="nestedUserQueue" create="true">
    <rule name="primaryGroup" create="false"/>
    </rule>
    <!-- 最后一个规则必须为 reject 或者 default。Reject 表示拒绝创建提交失败，
   default 表示把任务提交到 default 队列 -->
    <rule name="reject" />
    </queuePlacementPolicy>
   </allocations>
   ```

3. 分发配置并重启 Yarn

### 4.4 Yarn 的 Tool 接口案例

1. 回顾：期望可以动态传参，结果报错，误认为是第一个输入参数。

   ```bash
   [itwork@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.itwork.mapreduce.wordcount2.WordCountDriver /input /output1
   [itwork@hadoop102 hadoop-3.1.3]$ hadoop jar wc.jar com.itwork.mapreduce.wordcount2.WordCountDriver -D mapreduce.job.queuename=root.test /input /output1
   ```

2. 需求：自己写的程序也可以动态修改参数。编写 Yarn 的 Tool 接口。

3. 具体步骤：

   1. 新建 Maven 项目 YarnDemo，pom 如下：

   2. 新建 com.itwork.yarn 包名

   3. 创建类 WordCount 并实现 Tool 接口

      ```
      
      ```




## 5. Zookeeper

### 5.1 Zookeeper 本地安装

1. 安装前准备

   1. 安装 JDK

   2. 拷贝 apache-zookeeper-3.5.7-bin.tar.gz 安装包到 Linux `/opt/software/`系统下

   3. 解压到指定目录

      ```bash
      [lucp@hadoop102 software]$ tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/
      ```

   4. 修改名称

      ```bash
      [lucp@hadoop102 module]$ mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7
      ```

2. 配置修改

   1. 将/opt/module/zookeeper-3.5.7/conf 这个路径下的 zoo_sample.cfg 修改为 zoo.cfg；

      ```bash
      [lucp@hadoop102 module]$ cd /opt/module/zookeeper-3.5.7/conf
      [lucp@hadoop102 conf]$ mv zoo_sample.cfg zoo.cfg
      ```

   2. 打开 zoo.cfg 文件，修改 dataDir 路径：

      ```bash
      [lucp@hadoop102 conf]$ vim zoo.cfg
      ```

      ```
      # The number of milliseconds of each tick
      tickTime=2000
      # The number of ticks that the initial 
      # synchronization phase can take
      initLimit=10
      # The number of ticks that can pass between 
      # sending a request and getting an acknowledgement
      syncLimit=5
      # the directory where the snapshot is stored.
      # do not use /tmp for storage, /tmp here is just 
      # example sakes.
      
      #---------------------23-04-18 配置修改--------------------------
      dataDir=/opt/module/zookeeper-3.5.7/zkData
      #----------------------------------------------------------------
      
      # the port at which the clients will connect
      clientPort=2181
      # the maximum number of client connections.
      # increase this if you need to handle more clients
      #maxClientCnxns=60
      #
      # Be sure to read the maintenance section of the 
      # administrator guide before turning on autopurge.
      #
      # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
      #
      # The number of snapshots to retain in dataDir
      #autopurge.snapRetainCount=3
      # Purge task interval in hours
      # Set to "0" to disable auto purge feature
      #autopurge.purgeInterval=1
      ```

   3. 在/opt/module/zookeeper-3.5.7/这个目录上创建 zkData 文件夹

      ```bash
      [lucp@hadoop102 conf]$ cd /opt/module/zookeeper-3.5.7/
      [lucp@hadoop102 zookeeper-3.5.7]$ mkdir zkData
      ```

3. 操作 Zookeeper

   1. 启动 Zookeeper

      ```bash
      [lucp@hadoop102 zookeeper-3.5.7]$ bin/zkServer.sh start
      ZooKeeper JMX enabled by default
      Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg
      Starting zookeeper ... STARTED
      ```

   2. 查看进程是否启动

      ```bash
      [lucp@hadoop102 zookeeper-3.5.7]$ jps
      10053 QuorumPeerMain
      10086 Jps
      ```

   3. 查看状态

      ```bash
      [lucp@hadoop102 zookeeper-3.5.7]$ bin/zkServer.sh status
      ZooKeeper JMX enabled by default
      Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg
      Client port found: 2181. Client address: localhost.
      Mode: standalone
      ```

   4. 启动客户端

      ```bash
      [lucp@hadoop102 zookeeper-3.5.7]$ bin/zkCli.sh
      Connecting to localhost:2181
      2023-04-19 09:16:42,474 [myid:] - INFO  [main:Environment@109] - Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT
      2023-04-19 09:16:42,476 [myid:] - INFO  [main:Environment@109] - Client environment:host.name=hadoop102
      ...
      WATCHER::
      
      WatchedEvent state:SyncConnected type:None path:null
      ```

   5. 退出客户端：

      ```bash
      [zk: localhost:2181(CONNECTED) 0] quit
      ```

   6. 停止 Zookeeper

      ```bash
      [lucp@hadoop102 zookeeper-3.5.7]$ bin/zkServer.sh stop
      ZooKeeper JMX enabled by default
      Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg
      Stopping zookeeper ... STOPPED
      ```

### 5.2 Zookeeper 集群操作

#### 5.2.1 集群安装

1. 解压安装

   1. 在 hadoop102 解压 Zookeeper 安装包到/opt/module/目录下

      ```bash
      [lucp@hadoop102 software]$ tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/
      ```

   2. 修改 apache-zookeeper-3.5.7-bin 名称为 zookeeper-3.5.7

      ```bash
      [lucp@hadoop102 module]$ mv apache-zookeeper-3.5.7-bin/ zookeeper-3.5.7
      ```

2. 配置服务器编号

   1. 在/opt/module/zookeeper-3.5.7/这个目录下创建 zkData

      ```
      [lucp@hadoop102 conf]$ cd /opt/module/zookeeper-3.5.7/
      [lucp@hadoop102 zookeeper-3.5.7]$ mkdir zkData
      ```

   2. 在/opt/module/zookeeper-3.5.7/zkData 目录下创建一个 myid 的文件

      ```bash
      vim myid
      ```

      在文件中添加与 server 对应的编号（注意：上下不要有空行，左右不要有空格）

      ```
      2
      ```

   3. 拷贝配置好的 zookeeper 到其他机器上

      ```bash
      [itwork@hadoop102 module ]$ xsync zookeeper-3.5.7
      ```

      > 并分别在 hadoop103、hadoop104 上修改 myid 文件中内容为 3、4

3. 配置zoo.cfg文件

   1. 重命名/opt/module/zookeeper-3.5.7/conf 这个目录下的 zoo_sample.cfg 为 zoo.cfg

      ```
      [itwork@hadoop102 conf]$ mv zoo_sample.cfg zoo.cfg
      ```

   2. 打开 zoo.cfg 文件

      ```
      [itwork@hadoop102 conf]$ vim zoo.cfg
      ```

      \#修改数据存储路径配置

      ```
      dataDir=/opt/module/zookeeper-3.5.7/zkData
      ```

      \#增加如下配置

      ```
      #######################cluster##########################
      server.2=hadoop102:2888:3888
      server.3=hadoop103:2888:3888
      server.4=hadoop104:2888:3888
      ```

   3. 同步 zoo.cfg 配置文件

      ```
      [itwork@hadoop102 conf]$ xsync zoo.cfg
      ```

4. 集群操作

   1. 分别启动 Zookeeper

      ```bash
      [itwork@hadoop102 zookeeper-3.5.7]$ bin/zkServer.sh start
      [itwork@hadoop103 zookeeper-3.5.7]$ bin/zkServer.sh start
      [itwork@hadoop104 zookeeper-3.5.7]$ bin/zkServer.sh start
      ```

   2. 查看状态

      ```bash
      [itwork@hadoop102 zookeeper-3.5.7]# bin/zkServer.sh status
      JMX enabled by default
      Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg
      Mode: follower
      [itwork@hadoop103 zookeeper-3.5.7]# bin/zkServer.sh status
      JMX enabled by default
      Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg
      Mode: leader
      [itwork@hadoop104 zookeeper-3.4.5]# bin/zkServer.sh status
      JMX enabled by default
      Using config: /opt/module/zookeeper-3.5.7/bin/../conf/zoo.cfg
      Mode: follower
      ```

#### 5.2.2 选举机制（面试重点）

1. Zookeeper选举机制——第一次启动 ![image-20230419154638431](https://s2.loli.net/2023/04/19/vsKlPxakycwoFSz.png)
2. Zookeeper选举机制———非第一次启动 ![image-20230419154738729](https://s2.loli.net/2023/04/19/ymOHJhrDqcfUGT8.png)

#### 5.2.3 ZK 集群启动停止脚本

1. 在 hadoop102 的/home/lucp/bin 目录下创建脚本

   ```bash
   [lucp@hadoop102 bin]$ vim zk.sh
   ```

   在脚本中编写如下内容

   ```
   #!/bin/bash
   case $1 in
   "start"){
   for i in hadoop102 hadoop103 hadoop104
   do
    echo ---------- zookeeper $i 启动 ------------
   ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh 
   start"
   done
   };;
   "stop"){
   for i in hadoop102 hadoop103 hadoop104
   do
    echo ---------- zookeeper $i 停止 ------------ 
   ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh 
   stop"
   done
   };;
   "status"){
   for i in hadoop102 hadoop103 hadoop104
   do
    echo ---------- zookeeper $i 状态 ------------ 
   ssh $i "/opt/module/zookeeper-3.5.7/bin/zkServer.sh 
   status"
   done
   };;
   esac
   ```

2. 增加脚本执行权限

   ```bash
   [lucp@hadoop102 bin]$ chmod 777 zk.sh
   ```

3. Zookeeper 集群启动脚本 

   ```bash
   [lucp@hadoop102 bin]$ zk.sh start
   ```

4. Zookeeper 集群停止脚本 

   ```bash
   [lucp@hadoop102 bin]$ zk.sh stop
   ```

#### 5.2.4 节点类型（持久/短暂/有序号/无序号）

1. 分别创建2个普通节点（永久节点 + 不带序号）

   ```bash
   [zk: hadoop102:2181(CONNECTED) 10] create /xiyouji
   Created /xiyouji
   [zk: hadoop102:2181(CONNECTED) 12] ls /
   [xiyouji, zookeeper]
   [zk: hadoop102:2181(CONNECTED) 14] create /xiyouji/gaolaozhuang
   Created /xiyouji/gaolaozhuang
   ```

   > 注意：创建节点时，要赋值

2. 获得节点的值

   ```bash
   [zk: localhost:2181(CONNECTED) 0] get -s /xiyouji
   null
   cZxid = 0x100000004
   ctime = Wed Apr 19 10:28:24 CST 2023
   mZxid = 0x100000004
   mtime = Wed Apr 19 10:28:24 CST 2023
   pZxid = 0x100000006
   cversion = 2
   dataVersion = 0
   aclVersion = 0
   ephemeralOwner = 0x0
   dataLength = 0
   numChildren = 2
   ```

3. 创建带序号的节点（永久节点 + 带序号）

   ```bash
   [zk: localhost:2181(CONNECTED) 7] create /xiyouji
   Created /xiyouji
   [zk: localhost:2181(CONNECTED) 8] create -s /xiyouji/huaguoshan
   Created /xiyouji/huaguoshan0000000000
   [zk: localhost:2181(CONNECTED) 9] create -s /xiyouji/liushahe
   Created /xiyouji/liushahe0000000001
   [zk: localhost:2181(CONNECTED) 11] create -s /xiyouji/huaguoshan0000000000/shuiliandong
   Created /xiyouji/huaguoshan0000000000/shuiliandong0000000000
   ```

   > 如果原来没有序号节点，序号从 0 开始依次递增。如果原节点下已有 2 个节点，则再排 序时从 2 开始，以此类推。

4. 创建短暂节点（短暂节点 + 不带序号 or 带序号）

   1. 创建短暂的不带序号的节点

      ```bash
      [zk: localhost:2181(CONNECTED) 12] create -e /xiyouji/pansidong "zhizhujing"
      Created /xiyouji/pansidong
      ```

   2. 创建短暂的带序号的节点

      ```bash
      [zk: localhost:2181(CONNECTED) 13] create -e -s /xiyouji/pansidong "zhizhujing"
      Created /xiyouji/pansidong0000000003
      ```

   3. 在当前客户端是能查看到的

      ```bash
      [zk: localhost:2181(CONNECTED) 16] ls /xiyouji
      [huaguoshan0000000000, liushahe0000000001, pansidong, pansidong0000000003]
      ```

   4. 退出当前客户端然后再重启客户端

      ```bash
      [zk: localhost:2181(CONNECTED) 17] quit
      
      WATCHER::
      
      WatchedEvent state:Closed type:None path:null
      2023-04-19 10:49:05,996 [myid:] - INFO  [main:ZooKeeper@1422] - Session: 0x200001ec5410001 closed
      2023-04-19 10:49:05,996 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@524] - EventThread shut down for session: 0x200001ec5410001
      [lucp@hadoop102 zookeeper-3.5.7]$ bin/zkCli.sh
      ```

   5. 再次查看根目录下短暂节点已经删除

      ```bash
      [zk: localhost:2181(CONNECTED) 0] ls /xiyouji
      [huaguoshan0000000000, liushahe0000000001]
      ```

5. 修改节点数据值

#### 5.2.5 监听器原理

客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、节点删除、子目 录节点增加删除）时，ZooKeeper 会通知客户端。监听机制保证 ZooKeeper 保存的任何的数 据的任何改变都能快速的响应到监听了该节点的应用程序。![image-20230419111935478](https://s2.loli.net/2023/04/19/Zv8uoMazW25pfSt.png)



1. 节点的值变化监听

   1. 在 **hadoop104** 主机上注册监听 `/xiyouji/liushahe` 节点数据变化

      ```bash
      [lucp@hadoop104: zk: localhost:2181(CONNECTED) 9] get -w /xiyouji/liushahe
      shaheshang
      ```

   2. 在 hadoop103 主机上修改/sanguo 节点的数据

      ```bash
      [lucp@hadoop103: zk: localhost:2181(CONNECTED) 2] set /xiyouji/liushahe "shawujing"
      ```

   3. 观察 hadoop104 主机收到数据变化的监听

      ```bash
      [lucp@hadoop104: zk: localhost:2181(CONNECTED) 10] 
      WATCHER::
      
      WatchedEvent state:SyncConnected type:NodeDataChanged path:/xiyouji/liushahe
      ```

2. 节点的子节点变化监听（路径变化）

   1. 在 hadoop104 主机上注册监听/sanguo 节点的子节点变化

      ```bash
      [[itwork@hadoop104: zk: localhost:2181(CONNECTED) 1] ls -w /sanguo
      [shuguo, weiguo]
      ```

   2. 在 hadoop103 主机/sanguo 节点上创建子节点

      ```bash
      [itwork@hadoop103: zk: localhost:2181(CONNECTED) 2] create /sanguo/jin "simayi"
      Created /sanguo/jin
      ```

   3. 观察 hadoop104 主机收到子节点变化的监听

      ```bash
      WATCHER::
      WatchedEvent state:SyncConnected type:NodeChildrenChanged 
      path:/sanguo
      ```

#### 5.2.6 节点删除与查看

1. 删除节点

   ```bash
   [zk: localhost:2181(CONNECTED) 4] delete /sanguo/jin
   ```

2. 递归删除节点

   ```bash
   [zk: localhost:2181(CONNECTED) 15] deleteall /sanguo/shuguo
   ```

3. 查看节点状态

   ```bash
   [zk: localhost:2181(CONNECTED) 17] stat /sanguo
   cZxid = 0x100000003
   ctime = Wed Aug 29 00:03:23 CST 2018
   mZxid = 0x100000011
   mtime = Wed Aug 29 00:21:23 CST 2018
   pZxid = 0x100000014
   cversion = 9
   dataVersion = 1
   aclVersion = 0
   ephemeralOwner = 0x0
   dataLength = 4
   numChildren = 1
   ```

### 5.3 客户端 API 操作

#### 5.3.1 IDEA 环境搭建

1. 创建一个maven工程：Zookeeper

2. 更改Maven配置![image-20230419114452675](https://s2.loli.net/2023/04/19/vXIOGTfbKxQl3Ni.png)

3. 添加pom文件

4. 拷贝log4j.properties文件到项目根目录

   需要在项目的 src/main/resources 目录下，新建一个文件，命名为“log4j.properties”，在 文件中填入。

   ```
   
   ```

5. 创建包名
6. 创建类名称

#### 5.4.2 创建ZooKeeper类

1. 创建子节点

   ```java
   package com.lucp.zk;
   
   import org.apache.zookeeper.*;
   import org.junit.Before;
   import org.junit.Test;
   
   import java.io.IOException;
   
   public class zkClient {
   
       private ZooKeeper zkClient;
       private String connectString = "hadoop102:2181,hadoop103:2181,hadoop104:2181";
       private int sessionTimeout = 2000;
       @Before
       public void init() throws IOException {
           zkClient = new ZooKeeper(connectString, sessionTimeout, new Watcher() {
               @Override
               public void process(WatchedEvent watchedEvent) {
                   try {
                       zkClient.getChildren("/", true);
                   } catch (KeeperException | InterruptedException e) {
                       throw new RuntimeException(e);
                   }
               }
           });
       }
       // 创建子节点
       @Test
       public void create() throws Exception {
           // 参数 1：要创建的节点的路径； 参数 2：节点数据 ； 参数 3：节点权限 ；参数 4：节点的类型
           String nodeCreated = zkClient.create("/xiyouji/APITest", "sunwukong".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
       }
   }
   ```

   测试：在 hadoop102 的 zk 客户端上查看创建节点情况

   ```bash
   [lucp@hadoop102: zk: localhost:2181(CONNECTED) 8] get -s /xiyouji_APITest
   sunwukong
   ```

2. 获取子节点并监听节点变化

   ```java
   // 获取子节点
       @Test
       public void getChildren() throws Exception {
           List<String> children = zkClient.getChildren("/", true);
           for (String child : children) {
               System.out.println(child);
           }
           // 延时阻塞
           Thread.sleep(Long.MAX_VALUE);
       }
   ```

   在控制台看到如下节点

   ```
   xiyouji
   xiyouji_APITest
   zookeeper
   ```

   1. 在 hadoop102 的客户端上创建再创建一个节点/lucp，观察 IDEA 控制台

      ```bash
      [lucp@hadoop102: zk: localhost:2181(CONNECTED) 12] create /lucp "lucp"
      Created /lucp
      ```

      ```
      xiyouji
      xiyouji_APITest
      zookeeper
      xiyouji
      xiyouji_APITest
      zookeeper
      lucp
      ```

   2. 在 hadoop102 的客户端上删除节点/lucp，观察 IDEA 控制台

      ```
      [lucp@hadoop102: zk: localhost:2181(CONNECTED) 23] delete /lucp
      ```

      ```
      xiyouji
      xiyouji_APITest
      zookeeper
      xiyouji
      xiyouji_APITest
      zookeeper
      lucp
      xiyouji
      xiyouji_APITest
      zookeeper
      ```

3. 判断 Znode 是否存在

   ```java
   @Test
       public void exist() throws Exception {
           Stat stat = zkClient.exists("/lucp", false);
           System.out.println(stat == null ? "not exist" : "exist");
       }
   ```

   ```
   not exist
   ```

### 5.4 客户端向服务端写数据流程

### 5.5 服务器动态上下线监听案例

### 5.6 ZooKeeper 分布式锁案例

![image-20230420105853817](https://s2.loli.net/2023/04/20/VX9ZHLguxMmel1a.png)



## 6. Hive

### 6.1 Hive 入门

#### 6.1.1 什么是Hive

#### 6.1.2 Hive 架构原理

![image-20230421094718818](https://s2.loli.net/2023/04/21/lLNyR9nZQS57EaF.png)

### 6.2 Hive 安装

#### 6.2.1 Hive安装地址

1. 官网地址[Apache Hive](https://hive.apache.org/)
2. 文档查看地址[GettingStarted - Apache Hive - Apache Software Foundation](https://cwiki.apache.org/confluence/display/Hive/GettingStarted)
3. 下载地址[Index of /dist/hive (apache.org)](http://archive.apache.org/dist/hive/)
4. GitHub地址[GitHub - apache/hive: Apache Hive](https://github.com/apache/hive)

#### 6.2.2 Hive 安装部署

1. 安装Hive

   ```bash
   [lucp@hadoop102 home]$ cd /opt/software/
   [lucp@hadoop102 software]$ tar -zxvf /opt/software/apache-hive-3.1.3-bin.tar.gz -C /opt/module/
   
   [lucp@hadoop102 software]$ mv /opt/module/apache-hive-3.1.3-bin/ /opt/module/hive
   
   [lucp@hadoop102 software]$ sudo vim /etc/profile.d/my_env.sh
   
   [lucp@hadoop102 module]$ cd hive/
   [lucp@hadoop102 hive]$ source /etc/profile.d/my_env.sh
   [lucp@hadoop102 hive]$ bin/schematool -dbType derby -initSchema
   
   Initialization script completed
   schemaTool completed
   
   [lucp@hadoop102 hive]$ myhadoop.sh start
   
   [lucp@hadoop102 hive]$ bin/hive
   ```

   > 下面步骤建议不做

   ```bash
   hive> show databases;
   OK
   default
   Time taken: 0.738 seconds, Fetched: 1 row(s)
   
   hive> show tables;
   OK
   Time taken: 0.038 seconds
   
   hive> create table stu(id int,name string);
   OK
   Time taken: 0.671 seconds
   
   hive> insert into stu values(1,"ss");
   Query ID = lucp_20230421112001_bb69d018-48a9-4367-9e66-267fece052a6
   Total jobs = 3
   Launching Job 1 out of 3
   Number of reduce tasks determined at compile time: 1
   In order to change the average load for a reducer (in bytes):
     set hive.exec.reducers.bytes.per.reducer=<number>
   In order to limit the maximum number of reducers:
     set hive.exec.reducers.max=<number>
   In order to set a constant number of reducers:
     set mapreduce.job.reduces=<number>
   Starting Job = job_1682045210471_0001, Tracking URL = http://hadoop103:8088/proxy/application_1682045210471_0001/
   Kill Command = /opt/module/hadoop-3.1.3/bin/mapred job  -kill job_1682045210471_0001
   Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
   2023-04-21 11:20:12,710 Stage-1 map = 0%,  reduce = 0%
   2023-04-21 11:20:21,572 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.94 sec
   2023-04-21 11:20:25,731 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.17 sec
   MapReduce Total cumulative CPU time: 3 seconds 170 msec
   Ended Job = job_1682045210471_0001
   Stage-4 is selected by condition resolver.
   Stage-3 is filtered out by condition resolver.
   Stage-5 is filtered out by condition resolver.
   Moving data to directory hdfs://hadoop102:8020/user/hive/warehouse/stu/.hive-staging_hive_2023-04-21_11-20-01_053_5831516703813695029-1/-ext-10000
   Loading data to table default.stu
   MapReduce Jobs Launched: 
   Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.17 sec   HDFS Read: 15184 HDFS Write: 235 SUCCESS
   Total MapReduce CPU Time Spent: 3 seconds 170 msec
   OK
   Time taken: 26.242 seconds
   
   hive> select * from stu;
   OK
   1       ss
   Time taken: 0.167 seconds, Fetched: 1 row(s)
   
   hive> quit;
   
   [lucp@hadoop102 hive]$ rm -rf derby.log metastore_db/
   [lucp@hadoop102 hive]$ hadoop fs -rm -r /user
   Deleted /user
   ```

#### 6.2.3 MySQL 安装

```bash
[lucp@hadoop102 hive]$ cd /opt/software/
[lucp@hadoop102 software]$ mkdir mysql_lib
[lucp@hadoop102 software]$ tar -xf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar -C mysql_lib/
```

```bash
[lucp@hadoop102 mysql_lib]$ sudo rpm -ivh mysql-community-common-5.7.28-1.el7.x86_64.rpm
警告：mysql-community-common-5.7.28-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY
准备中...                          ################################# [100%]
正在升级/安装...
   1:mysql-community-common-5.7.28-1.e################################# [100%]

[lucp@hadoop102 mysql_lib]$ sudo rpm -ivh mysql-community-libs-5.7.28-1.el7.x86_64.rpm 
警告：mysql-community-libs-5.7.28-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY
准备中...                          ################################# [100%]
正在升级/安装...
   1:mysql-community-libs-5.7.28-1.el7################################# [100%]

[lucp@hadoop102 mysql_lib]$ sudo rpm -ivh mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm 
警告：mysql-community-libs-compat-5.7.28-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY
准备中...                          ################################# [100%]
正在升级/安装...
   1:mysql-community-libs-compat-5.7.2################################# [100%]

[lucp@hadoop102 mysql_lib]$ sudo rpm -ivh mysql-community-client-5.7.28-1.el7.x86_64.rpm 
警告：mysql-community-client-5.7.28-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY
准备中...                          ################################# [100%]
正在升级/安装...
   1:mysql-community-client-5.7.28-1.e################################# [100%]

[lucp@hadoop102 mysql_lib]$ sudo rpm -ivh mysql-community-server-5.7.28-1.el7.x86_64.rpm 
警告：mysql-community-server-5.7.28-1.el7.x86_64.rpm: 头V3 DSA/SHA1 Signature, 密钥 ID 5072e1f5: NOKEY
准备中...                          ################################# [100%]
正在升级/安装...
   1:mysql-community-server-5.7.28-1.e################################# [100%]

[lucp@hadoop102 mysql_lib]$ sudo systemctl start mysqld
[lucp@hadoop102 mysql_lib]$ sudo cat /var/log/mysqld.log | grep password
2023-04-21T05:40:20.399754Z 1 [Note] A temporary password is generated for root@localhost: rFf>8iY?VhIa
[lucp@hadoop102 mysql_lib]$ mysql -uroot -p'rFf>8iY?VhIa'
```

```bash
mysql> set password=password("pL&57#F@tL");
Query OK, 0 rows affected, 1 warning (0.00 sec)

mysql> set global validate_password_policy=0;
Query OK, 0 rows affected (0.00 sec)

mysql> set global validate_password_length=4;
Query OK, 0 rows affected (0.00 sec)

mysql> set password=password("123456");
Query OK, 0 rows affected, 1 warning (0.00 sec)

mysql> use mysql
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

mysql> select user, host from user;
+---------------+-----------+
| user          | host      |
+---------------+-----------+
| mysql.session | localhost |
| mysql.sys     | localhost |
| root          | localhost |
+---------------+-----------+
3 rows in set (0.00 sec)

mysql> update user set host="%" where user="root";
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> flush privileges;
Query OK, 0 rows affected (0.00 sec)

mysql> create database metastore;
Query OK, 1 row affected (0.00 sec)

mysql> quit
Bye
```

#### 6.2.4 配置 Hive 元数据存储到 MySQL

```bash
[lucp@hadoop102 mysql_lib]$ cd ..
[lucp@hadoop102 software]$ cp /opt/software/mysql-connector-java-5.1.37.jar $HIVE_HOME/lib 
[lucp@hadoop102 software]$ vim $HIVE_HOME/conf/hive-site.xml
```

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 存储元数据mysql相关配置 -->
    <!-- jdbc 连接的 URL -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>
    </property>


    <!-- jdbc 连接的 Driver -->
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>

    <!-- jdbc 连接的 username -->
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <!-- jdbc 连接的 password -->
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
    </property>

    <!-- Hive 默认在 HDFS 的工作目录 -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

</configuration>
```

> Hadoop集群开启情况下执行

```bash
[lucp@hadoop102 software]$ cd $HIVE_HOME
[lucp@hadoop102 hive]$ pwd
/opt/module/hive
[lucp@hadoop102 hive]$ bin/schematool -dbType mysql -initSchema -verbose
No rows affected (0.001 seconds)
0: jdbc:mysql://hadoop102:3306/metastore> !closeall
Closing: 0: jdbc:mysql://hadoop102:3306/metastore?useSSL=false
beeline> 
beeline> Initialization script completed
schemaTool completed
```

验证元数据是否配置成功

```bash
[lucp@hadoop102 hive]$ bin/hive
which: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/hadoop-3.1.3/bin:/opt/module/hadoop-3.1.3/sbin:/home/lucp/.local/bin:/home/lucp/bin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 494d99de-88ae-466f-9e0e-f7b3f67b6322

Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Hive Session ID = a103f880-6cf9-4f01-80d9-eaaf5e896a30
hive> 
```

在窗口中开启另一个窗口开启 Hive（两个窗口都可以操作 Hive，没有异常）

```bash
[lucp@hadoop103 ~]$ cd /opt/module/hive
[lucp@hadoop103 hive]$ bin/hive
which: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/hadoop-3.1.3/bin:/opt/module/hadoop-3.1.3/sbin:/home/lucp/.local/bin:/home/lucp/bin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 494d99de-88ae-466f-9e0e-f7b3f67b6322

Logging initialized using configuration in jar:file:/opt/module/hive/lib/hive-common-3.1.3.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Hive Session ID = a103f880-6cf9-4f01-80d9-eaaf5e896a30
hive> 
```

#### 6.2.5 Hive 服务部署

```bash
[lucp@hadoop102 module]$ cd $HADOOP_HOME/etc/hadoop
[lucp@hadoop102 hadoop]$ vim core-site.xml 
```

```xml
<!--配置所有节点的 atguigu 用户都可作为代理用户-->
<property>
<name>hadoop.proxyuser.lucp.hosts</name>
<value>*</value>
</property>
<!--配置 atguigu 用户能够代理的用户组为任意组-->
<property>
<name>hadoop.proxyuser.lucp.groups</name>
<value>*</value>
</property>
<!--配置 atguigu 用户能够代理的用户为任意用户-->
<property>
<name>hadoop.proxyuser.lucp.users</name>
<value>*</value>
</property>
```

```
[lucp@hadoop102 hadoop]$ xsync core-site.xml 
```

```
[lucp@hadoop102 hadoop]$ cd /opt/module/hive/conf
[lucp@hadoop102 conf]$ vim hive-site.xml 
```

```xml
<!-- 指定 hiveserver2 连接的 host -->
<property>
	<name>hive.server2.thrift.bind.host</name>
	<value>hadoop102</value>
</property>
<!-- 指定 hiveserver2 连接的端口号 -->
<property>
	<name>hive.server2.thrift.port</name>
	<value>10000</value>
</property>
```

测试

```bash
[lucp@hadoop102 conf]$ cd ..
[lucp@hadoop102 hive]$ bin/hive --service hiveserver2
which: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/module/jdk1.8.0_212/bin:/opt/module/hadoop-3.1.3/bin:/opt/module/hadoop-3.1.3/sbin:/home/lucp/.local/bin:/home/lucp/bin:/opt/module/jdk1.8.0_212/bin:/opt/module/hadoop-3.1.3/bin:/opt/module/hadoop-3.1.3/sbin:/opt/module/hive/bin)
2023-04-21 15:32:38: Starting HiveServer2
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = c798b42f-a70c-4690-9f26-e8c7bcad924d

```

```bash
[lucp@hadoop102 hive]$ nohup bin/hiveserver2 >/dev/null 2>&1 &
[1] 37839
[lucp@hadoop102 hive]$ bin/beeline -u jdbc:hive2://hadoop102:10000 -n lucp（用户名）
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/hive/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://hadoop102:10000
Connected to: Apache Hive (version 3.1.3)
Driver: Hive JDBC (version 3.1.3)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.3 by Apache Hive
0: jdbc:hive2://hadoop102:10000> 
```

![image-20230421155717653](https://s2.loli.net/2023/04/21/ipfJDcLI4gWq2o8.png)

![image-20230421155518568](https://s2.loli.net/2023/04/21/ilA1fMY7ZVFwWQh.png)

```bash
[lucp@hadoop102 conf]$ pwd
/opt/module/hive/conf
[lucp@hadoop102 conf]$ sudo vim hive-site.xml 
[lucp@hadoop102 conf]$ hive --service metastore
```

编写Hive 服务启动脚本

```bash
[lucp@hadoop102 conf]$ vim $HIVE_HOME/bin/hiveservices.sh
```

```sh
#!/bin/bash
HIVE_LOG_DIR=$HIVE_HOME/logs
if [ ! -d $HIVE_LOG_DIR ]
then
  mkdir -p $HIVE_LOG_DIR
fi
#检查进程是否运行正常，参数 1 为进程名，参数 2 为进程端口
function check_process()
{
 pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')
 ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
 echo $pid
 [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
}
function hive_start()
{
 metapid=$(check_process HiveMetastore 9083)
 cmd="nohup hive --service metastore >$HIVE_LOG_DIR/metastore.log 2>&1 &"
 [ -z "$metapid" ] && eval $cmd || echo "Metastroe 服务已启动"
 server2pid=$(check_process HiveServer2 10000)
 cmd="nohup hiveserver2 >$HIVE_LOG_DIR/hiveServer2.log 2>&1 &"
 [ -z "$server2pid" ] && eval $cmd || echo "HiveServer2 服务已启动" 
}
function hive_stop()
{
  metapid=$(check_process HiveMetastore 9083)
 [ "$metapid" ] && kill $metapid || echo "Metastore 服务未启动"
 server2pid=$(check_process HiveServer2 10000)
 [ "$server2pid" ] && kill $server2pid || echo "HiveServer2 服务未启动" 
}
case $1 in
"start")
 hive_start
;;
"stop")
 hive_stop
;;
"restart")
 hive_stop
 sleep 2
 hive_start
;;
"status")
 check_process HiveMetastore 9083 >/dev/null && echo "Metastore 服务运行正常" || echo "Metastore 服务运行异常"
 check_process HiveServer2 10000 >/dev/null && echo "HiveServer2 服务运行正常" || echo "HiveServer2 服务运行异常"
;;
*)
 echo Invalid Args!
 echo 'Usage: '$(basename $0)' start|stop|restart|status'
;;
esac
```

```
chmod +x $HIVE_HOME/bin/hiveservices.sh
hiveservices.sh start
```

#### 6.2.6 Hive 使用技巧

创建一个表student，并插入1 条数据

```mysql
create table student (id int,name string);
insert into table student values (1,"lucp");
select * from student;
```

此处省略了好多，后面补

```bash
[lucp@hadoop102 conf]$ myhadoop.sh stop
 =================== 关闭 hadoop 集群 ===================
 --------------- 关闭 historyserver ---------------
 --------------- 关闭 yarn ---------------
Stopping nodemanagers
Stopping resourcemanager
 --------------- 关闭 hdfs ---------------
Stopping namenodes on [hadoop102]
Stopping datanodes
Stopping secondary namenodes [hadoop104]
[lucp@hadoop102 conf]$ cd /opt/module/hadoop-3.1.3/etc/hadoop
[lucp@hadoop102 hadoop]$ vim yarn-site.xml 
[lucp@hadoop102 hadoop]$ xsync yarn-site.xml 
==================== hadoop102 ====================
sending incremental file list

sent 64 bytes  received 12 bytes  152.00 bytes/sec
total size is 7,340  speedup is 96.58
==================== hadoop103 ====================
sending incremental file list
yarn-site.xml

sent 491 bytes  received 101 bytes  394.67 bytes/sec
total size is 7,340  speedup is 12.40
==================== Hadoop104 ====================
sending incremental file list
yarn-site.xml

sent 491 bytes  received 101 bytes  1,184.00 bytes/sec
total size is 7,340  speedup is 12.40
[lucp@hadoop102 hadoop]$ myhadoop.sh start
```

```bash
[lucp@hadoop102 bin]$ pwd
/opt/module/hive/bin
[lucp@hadoop102 bin]$ hiveservices.sh start
Metastroe 服务已启动
HiveServer2 服务已启动
```

### 6.3 DDL 数据定义

#### 6.3.1 数据库（database）

```sql
create table student (id int,name string);
insert into table student values (1,"lucp");
select * from student;
insert into table student values (2,"lucp1");
-- 创建一个数据库，不指定路径
-- 若不指定路径，其默认路径为${hive.metastore.warehouse.dir}/database_name.db
create database db_hive1;
-- 创建一个数据库，指定路径
create database db_hive2 location '/db_hive2';
-- 创建一个数据库，带有dbproperties
create database db_hive3 with dbproperties('create_date'='2022-11-18');
-- 删除空数据库
drop database db_hive2;
-- 删除非空数据库
drop database db_hive1 cascade;
-- 切换当前数据库
use db_hive2;
```

#### 6.3.2 表（table）



### 6.4 DML 数据操作

#### 6.4.1 Load

#### 6.4.2 Insert

#### 6.4.3 Export&Import

### 6.5 查询

### 6.6 综合案例练习（初级）

### 6.7 函数

### 6.8 综合案例练习（中级）

### 6.9 分区表和分桶表

### 6.10 文件格式和压缩

### 6.11 企业级调优





